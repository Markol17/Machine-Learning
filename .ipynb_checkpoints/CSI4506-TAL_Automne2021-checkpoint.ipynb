{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voIH94SzCpLX"
   },
   "source": [
    "# Notebook 7 - Traitement Automatique des Langues (TAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFFoM0leCpLa"
   },
   "source": [
    "CSI4506 Intelligence Artificielle \\\n",
    "Automne 2021 \\\n",
    "Version 1 (2020) preparée par Julian Templeton, Caroline Barrière et Joel Muteba.  Version 2 (2021) modifiée par Caroline Barrière."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zYW_hamCpLa"
   },
   "source": [
    "***INTRODUCTION***:  \n",
    "\n",
    "La Covid-19 a changé notre vie quotidienne et a eu un impact mondial. Une grande partie de ce qui se passe dans le monde nous est rapportée par les nouvelles, sous forme de texte.\n",
    "\n",
    "Dans ce notebook, nous allons exécuter des techniques de Traitement Automatique des Langues (TAL) sur un ensemble de nouvelles liées à Covid-19. Le terme utilisé en TAL pour référer à un ensemble de textes est le terme *corpus*.  Ainsi, nous utiliserons ce terme *corpus* pour désigner l'ensemble des textes.\n",
    "\n",
    "Nous ferons de la tokenisation, de la lemmatisation, de l'étiquetage des parties du discours (POS), de la suppression des mots vides et plus encore sur des phrases individuelles provenant des articles de presse (partie 1 du Notebook) et ensuite sur l'ensemble du corpus (partie 2 du Notebook). Vous utiliserez la bibliothèque de TAL [spaCy](https://spacy.io/) pour exécuter ces techniques TAL. Cette bibliothèque simplifie l'exécution de ces opérations complexes pour du texte dans une langue spécifiée.\n",
    "\n",
    "Cela vous permettra d'explorer la pipeline TAL et d'analyser le contenu de textes provenant du corpus. Vous allez également créer des nuages de mots basés sur le texte modifié du corpus entier. Cela nous permettra de visualiser les mots clés des articles afin de mieux comprendre quel contenu y est le plus important.\n",
    "\n",
    "Dans les vidéos du cours, nous avons aussi parlé de *reconnaissance d'entités nommées* (NER).  Cette tâche ne sera pas explorée dans le présent notebook, mais plutôt dans le prochain.\n",
    "\n",
    "Pour ce notebook, vous devrez installer les packages suivants en plus des bibliothèques précédemment utilisées:     \n",
    "1) [spaCy](https://spacy.io/usage): pip install -U spacy      \n",
    "2) spaCy's English package (change the command according to your environment, ex: python vs py): py -m spacy download en          \n",
    "3) [WordCloud](https://amueller.github.io/word_cloud/): pip install wordcloud      \n",
    "4) MultiDict: pip install multidict     \n",
    "\n",
    "**Note:** Dans le code ci-bas, des options seront montrées pour l'installation des divers packages si vous travaillez sur colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NGqbdC9CpLb"
   },
   "source": [
    "***DEVOIR***:  \\\n",
    "\n",
    "Parcourez le notebook en exécutant chaque cellule, une à la fois.\n",
    "\n",
    "Recherchez **(TO DO)** pour les tâches que vous devez effectuer. Ne modifiez pas le code en dehors des questions auxquelles vous êtes invité à répondre à moins que cela ne vous soit spécifiquement demandé. Une fois que vous avez terminé, signez le notebook (à la fin du notebook), modifiez son nom pour NumEtudiant-NomFamille-Notebook7.ipynb et soumettez-le.\n",
    "\n",
    "*Le notebook sera noté sur 25. \\\n",
    "Chaque **(TO DO)** est associé à un certain nombre de points.*\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5z0F6fKJhdM"
   },
   "outputs": [],
   "source": [
    "# Already installed on colab, you might need to install it locally\n",
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lw0UeFmlJj_L"
   },
   "outputs": [],
   "source": [
    "# Already installed on colab, you might need to install it locally\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-AvR31oJoEb"
   },
   "outputs": [],
   "source": [
    "# NOT installed on colab, should be installed\n",
    "!pip install multidict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67Yk6q5XCpLc"
   },
   "outputs": [],
   "source": [
    "# Before starting we will import every module that we will be using\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import multidict as multidict\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dq_3-B5qCpLc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The core spacy object that will be used for tokenization, lemmatization, POS Tagging, ...\n",
    "# Note that this is specifically for the English language and requires the English package to be installed\n",
    "# via pip to work as intended.\n",
    "\n",
    "# sp = spacy.load('en')\n",
    "\n",
    "# If the above causes an error after installing the package described in the introduction (point 2), \n",
    "# then install the package as below\n",
    "!spacy download en_core_web_sm\n",
    "sp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfjHajC-CpLd"
   },
   "source": [
    "**PARTIE 1 - Analyse des phrases**  \n",
    "\n",
    "Dans cette partie, nous utiliserons les modules de *spaCy* pour effectuer les différentes étapes du pipeline TAL sur des phrases du fichier inclus sur les articles de presse liés à la Covid-19 provenant de CBC. Le corpus (news.csv) est inclus avec cette soumission, mais les détails le concernant peuvent être trouvés [ici](https://www.kaggle.com/ryanxjhan/cbc-news-coronavirus-articles-march-26?select=news.csv). \n",
    "\n",
    "La première chose que nous allons faire est de charger le fichier dans un dataframe pandas et se donner une idée du contenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ezsTA7aiCpLd"
   },
   "outputs": [],
   "source": [
    "# Read the dataset, show top rows\n",
    "df = pd.read_csv(\"news.csv\")\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIp14nD0CpLe"
   },
   "source": [
    "Tout d'abord, nous afficherons le texte d'un article et copierons manuellement les phrases qui seront utilisées pour cette partie du Notebook. Notez que de nombreuses balises sont enregistrées dans l'ensemble de données, mais nous ne nous en soucierons pas pour le moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9OCIlZJCpLe"
   },
   "outputs": [],
   "source": [
    "df[\"text\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hYIvkeNCpLf"
   },
   "source": [
    "À partir de ce texte, nous sélectionnerons une phrase qui sera utilisée par les exemples fournis dans le notebook, *sentence_example*, et cinq phrases que vous utiliserez pour répondre à cinq questions dans cette section, *sentence1*, ... *sentence5*. Les phrases 4 et 5 sont les mêmes car cette phrase est idéale pour les deux questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btihPePfCpLf"
   },
   "outputs": [],
   "source": [
    "# Sentence to be used for running examples\n",
    "sentence_example = \"Government guidelines in Canada recommend that people stay at least two metres away from others as part of physical distancing measures to curb the spread of COVID-19.\"\n",
    "# Sentences to be used for future questions\n",
    "sentence1 = \"I think those are provocative and those are hypothesis- generating, but then they need to be tested in the field.\\\" Loeb is running such a field test himself — a randomized controlled trial of the use of medical versus N95 masks among health care workers to see if there is a difference in the transmission of COVID-19.\"\n",
    "sentence2 = \"The World Health Organization recommends that people wear masks if they are coughing and sneezing or if they are caring for someone who is sick.\"\n",
    "sentence3 = \"Will it help if everyone wears a mask?\"\n",
    "sentence4 = \"Infection control guidelines do recommend extra personal protective equipment (including N-95 respirators) to protect against airborne transmission for healthcare workers performing procedures that generate high concentrations of aerosolized particles, such as intubations, on COVID-19 patients, McGeer said.\"\n",
    "sentence5 = \"Infection control guidelines do recommend extra personal protective equipment (including N-95 respirators) to protect against airborne transmission for healthcare workers performing procedures that generate high concentrations of aerosolized particles, such as intubations, on COVID-19 patients, McGeer said.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vw3C_rORCpLf"
   },
   "source": [
    "Nous allons maintenant explorer comment spaCy peut être appliqué sur une phrase pour effectuer différentes opérations. \n",
    "\n",
    "Tout d'abord, nous passerons la phrase exemple dans notre objet spacy *sp* pour récupérer la tokenisation, la lemmatisation, les valeurs de dépendance, les parties du discours (Part-of-speech, POS), et plus encore de la phrase. Comme vous le verrez, spaCy rend ce processus très facile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cmsR1JuCpLf"
   },
   "outputs": [],
   "source": [
    "# Call our spaCy object to retrieve the results of running the sentence through the NLP Pipeline\n",
    "# Note that we can reuse the sp variable without redefining it.\n",
    "sentence_example_content = sp(sentence_example)\n",
    "for token in sentence_example_content:\n",
    "    print(\"Text: \" + str(token.text) + \" Lemma: \" + str(token.lemma_) + \" POS: \" + token.pos_ + \n",
    "          \" Dependency: \" + token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGZ4ND45N0GX"
   },
   "outputs": [],
   "source": [
    "# We will take a look at the dependency tree to view how the words relate to each other\n",
    "displacy.render(sentence_example_content, style='dep', jupyter=True, options={'distance': 120})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAwSaLN2CpLg"
   },
   "source": [
    "Dans le code ci-dessus, nous voyons que nous sommes en mesure d'accéder aux balises de l'arbre de dépendances pour chaque token en appelant *.dep_*. Cependant, nous pouvons aussi directement accéder aux éléments de l'arbre de dépendances (comme vu dans le code ci-dessous). Pour plus d'exemples sur la façon de naviguer dans les arbres de dépendances, vous pouvez jeter un œil à [quelques exemples officiels de spaCy](https://spacy.io/usage/linguistic-features#dependency-parse). Cependant, vous trouverez ci-dessous des détails suffisants pour réaliser ce notebook.\n",
    "\n",
    "En regardant l'arbre de dépendances ci-dessus, nous voyons que les mots ont des flèches pour représenter les relations. Chacun d'entre eux a une étiquette montrant le type de dépendance entre les jetons (tokens). Par exemple, \"government\" est étiqueté avec une dépendance *compound* du nom 'guidelines', ce qui montre que 'government guidelines' est un *compound* (nom composé).\n",
    "\n",
    "Dans le code, après avoir analysé et divisé le texte avec spaCy en tokens, il est possible d'accéder aux mots auxquels un token est connecté par des flèches (ses dépendants, appelés aussi ses enfants). Ceci est présenté dans le code ci-dessous.\n",
    "\n",
    "Notez que lors de l'accès à un nœud enfant, vous pouvez accéder aux propriétés de la même manière que vous le feriez pour un token spaCy (.pos_, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gj2v_9nTCpLg"
   },
   "outputs": [],
   "source": [
    "# Display how to access the dependency children within a dependency tree\n",
    "for token in sentence_example_content:\n",
    "    print(\"Current token: \" + token.text)\n",
    "    print(\"All children of this token:\", list(token.children))\n",
    "    print(\"Left children of this token:\", list(token.lefts))\n",
    "    print(\"Right children of this token:\", list(token.rights))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf7j-SiuCpLg"
   },
   "source": [
    "**(TO DO) Q1(a) - 1 point**  \n",
    "Utilisez spaCy pour faire passer la phrase *sentence1* au travers de la pipeline TAL et déterminer le nombre de tokens dans la phrase. Montrez aussi tous les tokens trouvés dans la phrase *sentence1*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njbjYdfICpLh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RÉPONSE Q1(a)\n",
    "# Show tokens and print how many tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cL1yGkedRaFy"
   },
   "source": [
    "**(TO DO) Q1(b) - 1 point**  \n",
    "Regardez les tokens générés. Les tokens proviennent généralement de la séparation d'une phrase à l'aide d'éléments séparateurs particuliers. Mais ce n'est pas déterministe. En particulier, regardez le tiret (-). Comment le tiret est-il traité ? Il y a deux tirets différents (-) dans le texte, exprimez pourquoi vous pensez qu'ils sont traités différemment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Odyn_5SvR1h1"
   },
   "source": [
    "**RÉPONSE - Q1(b)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1JB8Xl-CpLh"
   },
   "source": [
    "**(TO DO) Q2 - 2 points**  \n",
    "Pour *sentence2*, afficher l'arborescence des dépendances et déterminer quel est le sujet du verbe *recommends* (le nom au complet). Vous n'avez pas besoin de le faire automatiquement, imprimez simplement la valeur que vous trouvez en regardant l'arbre des dépendances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4av1we8uCpLh"
   },
   "outputs": [],
   "source": [
    "# RÉPONSE Q2\n",
    "# Display the dependency tree for sentence2\n",
    "...\n",
    "# What is the subject of the verb 'recommends' in sentence2  (just print it based on your observation of the dependency tree)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0ElDo63CpLh"
   },
   "source": [
    "**(TO DO) Q3 - 2 points**  \n",
    "Utilisez spaCy pour faire passer la phrase *sentence3* dans la pipeline TAL et n'imprimez que les mots qui sont des verbes (*VERB*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2-Cph69CpLh"
   },
   "outputs": [],
   "source": [
    "# ANSWER Q3 \n",
    "# Find the verbs in Sentence 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1v_ScbmCpLh"
   },
   "source": [
    "**(TO DO) Q4 - 4 points** \n",
    "\n",
    "a) (1 point) Utilisez spaCy pour faire passer la phrase *sentence4* dans la pipeline TAL et n'imprimez que les mots qui sont des adjectifs (*ADJ*).\n",
    "\n",
    "b) (3 points) Pour chaque adjectif trouvé en (a), trouvez les noms que l'adjectif modifie. \n",
    "\n",
    "Pour ce faire, vous devrez parcourir les balises de l'arbre de dépendances pour trouver les adjectifs avec la balise *amod* qui vous mèneront vers les noms qu'ils modifient.\n",
    "\n",
    "*Astuce*: Rappelez-vous de l'exemple au début de cette partie qui montre comment sélectionner un token et accéder à ses dépendants.\n",
    "\n",
    "Notez également que vous pouvez aborder ce problème de plusieurs façons, alors n'hésitez pas à concevoir l'approche vous-même (à condition qu'elle réponde correctement à la question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SoyY2lETZnvt"
   },
   "outputs": [],
   "source": [
    "# RÉPONSE Q4 (a)\n",
    "# Find the adjectives in Sentence 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niwI235lCpLi"
   },
   "outputs": [],
   "source": [
    "# Display the dependency tree       \n",
    "displacy.render(s4, style='dep', jupyter=True, options={'distance': 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OP-yFB7CpLi"
   },
   "outputs": [],
   "source": [
    "# RÉPONSE Q4 (b)\n",
    "# Print the nouns that an adjective modifies with the amod dependency label\n",
    "# Go through the spaCy tokens, look for a specific POS tag, find the amod relations and print the relationship\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ON8C9iX3CpLi"
   },
   "source": [
    "**(TO DO) Q5 - 3 points**  \n",
    "Utilisez spaCy pour faire passer la phrase *sentence5* au travers la pipeline TAL et trouver tous les *noms composés*. Un nom composé se compose d'un ou de plusieurs mots avec une valeur de dépendance *compound* (qui sont également des noms, *NOUN*) suivie d'un nom (*compound*, ..., *compound*, *non-compound NOUN*).\n",
    "\n",
    "Pour afficher les noms composés, vous pouvez afficher l'arbre des dépendances de la phrase après l'avoir exécutée dans la pipeline TAL via spaCy.\n",
    "\n",
    "Vous devez mettre les noms composés en entier pour que votre réponse soit considérée correcte. Imprimez les noms composés obtenus.\n",
    "\n",
    "Par exemple, *infection control guidelines* est un nom composé que vous devriez trouver avec votre code. Remarquez à quel point un tel terme est plus informatif que seulement *guidelines*. Trouver automatiquement des noms composés dans un texte nous aide à comprendre de quoi parle un texte.\n",
    "\n",
    "Notez que vous pouvez aborder ce problème de plusieurs façons, alors n'hésitez pas à concevoir l'approche vous-même (tant qu'elle répond correctement à la question).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_i5e1EyNCpLi"
   },
   "source": [
    "Ci-dessous, nous avons analysé la phrase, imprimé les noms composés et affiché l'arborescence de dépendances que vous pouvez consulter avant de coder dans la cellule suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z86MkGslCpLi"
   },
   "outputs": [],
   "source": [
    "# Apply spaCy to sentence5\n",
    "s5 = sp(sentence5)\n",
    "\n",
    "# Display all tokens having a compound dependency within the sentence\n",
    "for token in s5:\n",
    "    if token.dep_ == \"compound\":\n",
    "        print(token)\n",
    "\n",
    "# Display the dependency tree\n",
    "displacy.render(s5, style='dep', jupyter=True, options={'distance': 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X9EjCyEnbZ8V"
   },
   "outputs": [],
   "source": [
    "# RÉPONSE Q5\n",
    "# Find and connect all noun compounds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNyPkZKDCpLj"
   },
   "source": [
    "**PARTIE 2 - Analyse du corpus**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGVfwgUCCpLj"
   },
   "source": [
    "Pour la deuxième section de ce notebook, nous nous concentrerons sur l'analyse de l'ensemble du corpus en créant des nuages de mots basés sur le contenu du corpus. Cela nous aidera à identifier les mots clés dans les articles en fonction des critères que nous appliquons aux données avec des techniques de TAL.\n",
    "\n",
    "Pour cette section, nous utiliserons la bibliothèque WordCloud qui nous permet de créer les nuages de mots avec du texte ou par fréquences des mots dans le texte. Le code pour générer les nuages de mots en fonction de la fréquence provient de cet [exemple WordCloud](https://amueller.github.io/word_cloud/auto_examples/frequency.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHDuGBdZCpLj"
   },
   "source": [
    "Nous commencerons par un exemple simple de création d'un nuage de mots basé sur les titres des documents de notre corpus. Bien que nous puissions utiliser les contenus entiers des documents, cela peut prendre trop de temps. Ainsi, nous travaillerons avec les titres qui permettront de générer des nuages de mots en environ une minute chacun (ou moins selon votre environnement).\n",
    "\n",
    "Nous allons créer un nuage de mots basé sur les fréquences de chaque terme à partir des titres de notre corpus, en appelant la fonction *getFrequencyDictForText* ci-dessous, et en passant ces fréquences au nuage de mots via la fonction *generate_from_frequencies* de WordCloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5G2IHuAdCpLj"
   },
   "outputs": [],
   "source": [
    "# Code from the example in: https://amueller.github.io/word_cloud/auto_examples/frequency.html\n",
    "def getFrequencyDictForText(sentence):\n",
    "    fullTermsDict = multidict.MultiDict()\n",
    "    tmpDict = {}\n",
    "    # making dict for counting frequencies\n",
    "    for text in sentence.split(\" \"):\n",
    "        val = tmpDict.get(text, 0)\n",
    "        tmpDict[text.lower()] = val + 1\n",
    "    for key in tmpDict:\n",
    "        fullTermsDict.add(key, tmpDict[key])\n",
    "    return fullTermsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mfZvFhJCpLj"
   },
   "outputs": [],
   "source": [
    "# This function comes from: https://towardsdatascience.com/simple-wordcloud-in-python-2ae54a9f58e5\n",
    "# Define a function to plot a word cloud\n",
    "def plot_cloud(wordcloud):\n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(40, 30))\n",
    "    # Display image\n",
    "    plt.imshow(wordcloud) \n",
    "    # No axis details\n",
    "    plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VuDlUSbKCpLj"
   },
   "outputs": [],
   "source": [
    "# This can take about a minute\n",
    "# Retrieve the frequencies from the titles in the dataframe\n",
    "frequencies = getFrequencyDictForText(' '.join(df[\"title\"]))\n",
    "# Create a word cloud based on the frequencies from the titles in the dataframe\n",
    "word_cloud = WordCloud(width=3000, height=2000, random_state=1, collocations=False).generate_from_frequencies(frequencies)\n",
    "# Plot the word cloud\n",
    "plot_cloud(word_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YL4q9RBCpLj"
   },
   "source": [
    "Comme vous pouvez le voir, nous pouvons facilement utiliser la fréquence des termes des titres pour créer un nuage de mots. Ces nuages de mots peuvent être personnalisés pour être dans des arrière-plans d'image, avoir des couleurs personnalisées, etc.\n",
    "\n",
    "Le nuage de mots ci-dessus contient des termes importants, mais la plupart des termes les plus fréquents ne sont pas très importants (ou sont des symboles/nombres). Ces termes qui apparaissent très fréquemment dans de nombreux types de documents, mais qui ne sont pas importants, sont appelés ***stopwords*** (***mots vides ou mots outils***). Par exemple, les mots *the*, *to* , *is*, *of*, ... sont des mots qui apparaissent extrêmement fréquemment dans le texte, mais qui ne fournissent aucune information significative quant au sujet traité dans le texte. Souvent, nous voulons supprimer ces mots vides. Pour cette raison, les bibliothèques de TAL, telles que spaCy, fournissent des méthodes pour détecter quels mots sont des mots vides. Vous trouverez ci-dessous un exemple de la façon dont spaCy peut être utilisé pour déterminer si un mot est un mot vide (basé sur la phrase utilisée dans le premier exemple de la partie 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tACOBuOCCpLj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call our spaCy object to retrieve the results of running the sentence through the NLP Pipeline\n",
    "# Note that we can reuse the sp variable without redefining it.\n",
    "sentence_example_content = sp(sentence_example)\n",
    "for token in sentence_example_content:\n",
    "    print(\"Text: \" + str(token.text) + \" Is stopword: \" + str(token.is_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_Oa-Q9lCpLj"
   },
   "source": [
    "Ainsi, dans les prochaines questions, vous explorerez différentes manières de manipuler les données de titre avant de générer les fréquences pour créer les nuages de mots. Cela se traduira par différents nuages de mots qui nous permettront de visualiser les termes importants du texte en fonction de certains critères.\n",
    "\n",
    "Pour les prochaines questions, vous pourrez vous référer aux exemples de la partie 1 qui montrent comment spaCy peut être utilisé pour effectuer la lemmatisation et le POS-tagging et l'exemple ci-dessus qui montre comment spaCy peut être utilisé pour effectuer la détection de mots vides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjzkzYwiCpLj"
   },
   "source": [
    "**(TO DO) Q6 - 2 points**    \n",
    "\n",
    "Créez un nuage de mots en fonction de la fréquence du contenu des titres des documents, là où les mots vides sont supprimés (vous devez utiliser spaCy pour trouver les mots vides).\n",
    "\n",
    "*Assurez-vous d'utiliser random_state = 1 lors de la génération du nuage de mots.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9LzBHkFaCpLk"
   },
   "outputs": [],
   "source": [
    "# RÉPONSE Q6 - \n",
    "# Get the titles and run them through spaCy\n",
    "...\n",
    "# Create a string of any token that is not a stopword\n",
    "...\n",
    "# Get the frequencies\n",
    "...\n",
    "# Create the word cloud (with random_state=1)\n",
    "...\n",
    "# Plot the word cloud\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzpKiVzhCpLk"
   },
   "source": [
    "**(TO DO) Q7 - 3 points**    \n",
    "\n",
    "a) (2 points) Créez un nuage de lemmes (un lemme est issu de la lemmatisation d'un token) basé sur la fréquence du contenu des titres des documents avec les mots vides supprimés, où les lemmes proviennent de spaCy.\n",
    "\n",
    "b) (1 point) Ensuite, comparez le nuage de mots résultant avec le nuage de mots généré à Q6. Quelle est la différence entre les deux?  Donnez des exemples spécifiques de différence.\n",
    "\n",
    "*Assurez-vous d'utiliser random_state = 1 lors de la génération du nuage de mots.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11x35g68CpLk"
   },
   "outputs": [],
   "source": [
    "# RÉPONSE Q7(a)\n",
    "# Make a lemma cloud\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgyjaxavCpLk"
   },
   "source": [
    "**RÉPONSE Q7 (b)**    \n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_3yI1fNCpLk"
   },
   "source": [
    "**(TO DO) Q8 - 3 points**    \n",
    "\n",
    "Construisez un nuage de mots basé sur le contenu des titres des documents où seuls les *adjectifs* sont utilisés ET où tous les mots vides sont supprimés ET où les lemmes sont ajoutés (plutôt que le texte).\n",
    "\n",
    "*Assurez-vous d'utiliser random_state = 1 lors de la génération du nuage de mots.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viFi7khXCpLk"
   },
   "outputs": [],
   "source": [
    "# RÉPONSE Q8\n",
    "# Word cloud with stopwords removed, only using the lemmas of the adjectives\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQ8r9WCVCpLk"
   },
   "source": [
    "**(TO DO) Q9 - 2 points**    \n",
    "\n",
    "En vous basant sur votre propre choix, créez un nuage de mots basé sur le contenu des titres des documents où seuls les *verbes* ou les *noms* sont utilisés (vous en sélectionnez un avec lequel travailler) ET où tous les mots vides sont supprimé ET où les lemmes sont ajoutés (plutôt que le texte).\n",
    "\n",
    "*Assurez-vous d'utiliser random_state = 1 lors de la génération du nuage de mots.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EwhctYgfCpLk"
   },
   "outputs": [],
   "source": [
    "# RÉPONSE Q9\n",
    "# Word cloud with either nouns or verbs only, stopwords removed, and lemmas used\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_eN5-43CpLk"
   },
   "source": [
    "\n",
    "Maintenant que tous les nuages de mots ont été créés, vous pouvez les comparer pour analyser comment les techniques de TAL qui ont été effectuées ont eu un impact sur les nuages de mots générés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkFpP9YXCpLk"
   },
   "source": [
    "**(TO DO) Q10 - 2 points**   \n",
    "\n",
    "Parmi les nuages de mots que vous avez créés, selon vous, quel nuage de mots a fourni les termes les plus pertinents liés à Covid-19 et pourquoi ? Donnez quelques exemples de termes que vous jugez pertinents. Donnez des exemples de termes qui ne vous semblent pas pertinents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5z4WL1R0CpLl"
   },
   "source": [
    "**RÉPONSE Q10**\n",
    "\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IimZIBT_CpLl"
   },
   "source": [
    "***SIGNATURE:***\n",
    "Mon nom est --------------------------.\n",
    "Mon numéro d'étudiant(e) est -----------------.\n",
    "Je certifie être l'auteur(e) de ce devoir."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CSI4506-TAL_Automne2021.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
