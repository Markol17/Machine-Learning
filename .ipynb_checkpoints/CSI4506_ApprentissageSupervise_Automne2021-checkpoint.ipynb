{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Bb7n9VJ-Nkt"
   },
   "source": [
    "# Notebook 4 - Apprentissage supervisé et le classifieur Naïf Bayesien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7nwPHSM-Nkx"
   },
   "source": [
    "CSI4506 Intelligence Artificielle  \n",
    "Automne 2021  \n",
    "Version 1 (2020) préparée par Julian Templeton, Caroline Barrière et Joel Muteba.  Version 2 (2021) préparée par Caroline Barrière."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFFz9Hxp-Nky"
   },
   "source": [
    "***DÉFINITION DU PROBLÈME***:\n",
    "\n",
    "La tâche de classification supervisée abordée dans ce notebook est la **détection de polarité**, qui est une activité possible dans la tendance très populaire de *Opinion Mining* dans l’IA.  Beaucoup d’entreprises veulent savoir s’il y a des commentaires positifs ou négatifs à leur sujet.  Les commentaires peuvent être sur les hôtels, restaurants, films, service à la clientèle de toute sorte, etc.\n",
    "\n",
    "Dans la détection de polarité, nous utilisons deux classes : ***positive*** et ***négative***.  C’est différent de l’analyse des sentiments par exemple, dans laquelle les classes pourraient être (triste, heureux, anxieux, en colère, etc.). La détection de polarité est également plus restreinte que le *rating* (notation) dans lequel nous aimerions attribuer une valeur (de 0 à 5 par exemple) pour évaluer un service particulier. .\n",
    "\n",
    "Dans ce notebook, nous utiliserons l’algorithme d’apprentissage automatique Naive Bayes (classifieur naïf bayesien) pour la détection de polarité d’un grand ensemble de données (dataset) de critiques de film.\n",
    "\n",
    "Un but important de ce notebook est de vous permettre de mieux comprendre comment mettre en place une **experimentation** pour l’apprentissage automatique/machine supervisé.  Nous regarderons la notion d’ensemble ou de données d'entraînement, de test, d'évaluation, etc.  Le notebook introduit également la notion d’évaluation comparative.  Pour dire si une méthode est bonne ou non, nous pouvons la comparer à une approche de *base*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmPUsGZD-1bY"
   },
   "source": [
    "***ENVIRONNEMENT DE PROGRAMMATION***:\n",
    "\n",
    "Ce notebook utilise un package d’apprentissage automatique vraiment populaire appelé **scikit-learn** (http://scikit-learn.org/stable/).  Il contient de nombreux algorithmes d’apprentissage automatique pré-codés que vous pouvez appeler.  \n",
    "\n",
    "L'algorithme de Naive Bayes fait partie de ce package *scikit-learn* que nous utiliserons.  Et dans de futurs notebooks, nous explorerons d’autres algorithmes d'apprentissage inclus dans scikit-learn.\n",
    "\n",
    "Pour utiliser ce package, vous devez le télécharger. Vous aurez également besoin de télécharger **Pandas** qui est un excellent outil pour manipuler les données à utiliser dans les algorithmes d’apprentissage automatique et **Numpy** qui est souvent utilisé par les librairies d’apprentissage automatique.  À l’invite de commandes, tapez ***pip install numpy***, ***pip install sklearn*** et ***pip install pandas*** pour télécharger les packages.\n",
    "\n",
    "ATTENTION:  Si vous rencontrez des problèmes d'installation pour ces packages scientifiques sur votre système local, je vous suggère fortement de passer à l'environnement en ligne fourni par Google, appelé Colab. Accédez simplement au site Colab (https://colab.research.google.com/) et vous pourrez y télécharger votre Notebook et l'exécuter. Tous les packages requis dans le notebook sont déjà installés dans l'environnement auquel vous aurez accès dans Colab. Mais si un package manquait, vous pourriez l'installer en ligne en incluant \"!pip install package_name\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpt7jkKW_IW0"
   },
   "source": [
    "***DONNÉES (DATASET) ET RESSOURCES***:\n",
    "\n",
    "Vous devrez télécharger l'ensemble de données (dataset) de critiques de films à partir du Google Drive partagé suivant:\n",
    "https://drive.google.com/file/d/1w1TsJB-gmIkZ28d1j7sf1sqcPmHXw352/view\n",
    "\n",
    "Les revues/critiques de film auront la mention Fresh (frais, donc positif) ou Rotten (pourri, donc négatif). Nous allons utiliser cet ensemble de données (dataset) dans tout le notebook alors soyez sûr de le placer dans le même répertoire que ce notebook. Il contient 480000 commentaires avec la moitié d’entre eux étant Rotten (pourri) et l’autre moitié étant Fresh (frais). Nous n’utiliserons qu’un sous-ensemble de ceux-ci en raison du temps de calcul important de l’apprenant de base.\n",
    "\n",
    "Si vous travaillez sur votre propre machine, vous pourrez accéder au fichier localement une fois que vous l'aurez téléchargé. Si vous décidez de travailler sur Colab, vous pouvez télécharger (upload) le dataset vers Colab (cliquez l'icône de dossier que vous voyez à gauche et vous pourrez alors faire un téléchargement de fichier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaVPSZjY-Nkz"
   },
   "source": [
    "***DEVOIR***:  \n",
    "\n",
    "Parcourez le notebook en exécutant chaque cellule, une à la fois.  \n",
    "Recherchez **(TO DO)** pour les tâches que vous devez effectuer. Ne modifiez pas le code en dehors des questions auxquelles on vous demande de répondre, sauf si spécifié. Une fois que vous avez terminé, signez le notebook (à la fin du notebook), renommez-le NumEtudiant-NomFamille-Notebook4.ipynb, et soumettez-le.  \n",
    "\n",
    "*Le notebook sera noté sur 25.  \n",
    "Chaque **(TO DO)** a un certain nombre de points qui lui sont associés.*\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCa6EmKG-Nk1"
   },
   "source": [
    "**1. Domaine d'application:  Revue de films**  \n",
    "\n",
    "Dans ce notebook, nous souhaitons appliquer la détection de polarité dans le domaine des films.  Les critiques/revues de films donnent une critique accompagnée d’un score pour les films qu’ils passent en revue. Le site Rotten Tomatoes est un site Web qui recueille les critiques de films et les cotes accompagnées. Les cotes peuvent être classés comme « Rotten » pour un faible score de revue ou « Fresh » pour un score de revue plus élevé. Nous utiliserons l'ensemble de données *rt_reviews.csv* que vous avez téléchargé précédemment pour effectuer la détection de polarité.\n",
    "\n",
    "La première chose à faire est de configurer ou former les ensembles d'entrainement et de test pour nos modèles. Nous allons construire ces ensembles en important les données de l'ensemble de données (dataset) à l’aide de pandas, puis utiliser le \"dataframe\" (type de données pandas) avec la fonction \"train_test_split\" de sckikit learn qui séparera les données en ensemble d'entrainement (training set) et ensemble de test (test set). Ceux-ci seront utilisés ultérieurement par les modèles qui seront créés/utilisés.\n",
    "\n",
    "Nous **NE DEVONS PAS** utiliser l'ensemble de test pour construire notre modèle. L’ensemble de test a pour but de tester le modèle après que nous l’ayons entrainé avec l’ensemble d'entrainement.  Dans la vidéo du cours (partie 4 - Évaluation, de la série sur l'apprentissage machine supervisé), je mentionne que cet ensemble est appelé normalement \"ensemble de validation\" mais sckit-learn l'appelle ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3xFMI5zX-Nk2"
   },
   "outputs": [],
   "source": [
    "# Import the libraries that we will use to help create the train and test sets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3875N0fTDYff"
   },
   "outputs": [],
   "source": [
    "# Put the dataset in a dataframe\n",
    "# Make sure you put rt_reviews.csv in your colab space, or locally\n",
    "df = pd.read_csv(\"rt_reviews.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhjLYOWZ-Nk4"
   },
   "source": [
    "La première étape après le chargement des données est de les regarder pour vous donner une idée de la tâche à accomplir et de la qualité de l'annotation fournie.  L'ensemble annoté est notre référence (\"gold standard\").  C'est ce que nous tenterons d'apprendre avec nos algorithmes.  Rappelez-vous que pour certaines tâches, il y a une dose de subjectivité.\n",
    "\n",
    "Pandas offre les deux fonctions utiles df.head() et df.tail() qui vous permettent de visualiser le haut et le bas de votre dataframe (cadre de données)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "ahXOtSes-Nk5",
    "outputId": "c2954678-79ae-4e1b-bdc7-72b98bdc575c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Freshness</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fresh</td>\n",
       "      <td>Manakamana doesn't answer any questions, yet makes its point: Nepal, like the rest of our plane...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fresh</td>\n",
       "      <td>Wilfully offensive and powered by a chest-thumping machismo, but it's good clean fun.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rotten</td>\n",
       "      <td>It would be difficult to imagine material more wrong for Spade than Lost &amp; Found.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rotten</td>\n",
       "      <td>Despite the gusto its star brings to the role, it's hard to ride shotgun on Hector's voyage of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rotten</td>\n",
       "      <td>If there was a good idea at the core of this film, it's been buried in an unsightly pile of fla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rotten</td>\n",
       "      <td>Gleeson goes the Hallmark Channel route, damaging an intermittently curious entry in the time t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fresh</td>\n",
       "      <td>It was the height of satire in 1976: dark as hell, but patently absurd and surely nowhere close...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rotten</td>\n",
       "      <td>Everyone in \"The Comedian\" deserves a better movie than \"The Comedian.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rotten</td>\n",
       "      <td>Actor encourages grumpy Christians to embrace the season.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fresh</td>\n",
       "      <td>Slight, contained, but ineffably soulful.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Freshness                                                                                               Review\n",
       "0     fresh   Manakamana doesn't answer any questions, yet makes its point: Nepal, like the rest of our plane...\n",
       "1     fresh                Wilfully offensive and powered by a chest-thumping machismo, but it's good clean fun.\n",
       "2    rotten                    It would be difficult to imagine material more wrong for Spade than Lost & Found.\n",
       "3    rotten   Despite the gusto its star brings to the role, it's hard to ride shotgun on Hector's voyage of ...\n",
       "4    rotten   If there was a good idea at the core of this film, it's been buried in an unsightly pile of fla...\n",
       "5    rotten   Gleeson goes the Hallmark Channel route, damaging an intermittently curious entry in the time t...\n",
       "6     fresh   It was the height of satire in 1976: dark as hell, but patently absurd and surely nowhere close...\n",
       "7    rotten                              Everyone in \"The Comedian\" deserves a better movie than \"The Comedian.\"\n",
       "8    rotten                                            Actor encourages grumpy Christians to embrace the season.\n",
       "9     fresh                                                            Slight, contained, but ineffably soulful."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 100\n",
    "df.head(10) # Show the first ten reviews of the dataset to understand the dataframe's structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ks2EmEpFI3T"
   },
   "source": [
    "**(TO DO) Q1 - 2 points** \\\n",
    "(a) Afficher les 20 dernières reviews de l'ensemble. \\\n",
    "(b) Trouvez des adjectifs utilisés dans les critiques qui indiqueraient (à votre avis) que le film était rotten ou fresh. Donnez 3-4 adjectifs avec leur association (fresh ou rotten). Par exemple, ci-dessus, \"good\" (review 1) est associé à fresh, ou \"wrong\" (review 2) est associé à rotten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LF3ZIdoaFs1q"
   },
   "outputs": [],
   "source": [
    "# RÉPONSE Q1 - Partie (a)\n",
    "#\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yuun_-_tFwRl"
   },
   "source": [
    "**RÉPONSE Q1 - Partie (b)**\\\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVCRB1o6F6k1"
   },
   "source": [
    "La deuxième étape consiste à prendre un échantillon aléatoire des films. L'ensemble de données est assez volumineux (480 000 revues), donc le temps de traitement sera assez long si on garde tout. Nous sélectionnerons 50000 au hasard. Si vous travaillez avec colab, 50K peut être raisonnable, mais si vous travaillez localement et que c'est trop grand, vous pouvez réduire l'échantillon à 10K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSpM7zYP-Nk5"
   },
   "outputs": [],
   "source": [
    "# Randomly select 10000 fresh examples from the dataframe\n",
    "dfFresh = df[df[\"Freshness\"] == \"fresh\"].sample(n=50000, random_state=8)\n",
    "# Randomly select 10000 rotten examples from the dataframe\n",
    "dfRotten = df[df[\"Freshness\"] == \"rotten\"].sample(n=50000, random_state=5)\n",
    "# Combine the results to make a small random subset of reviews to use\n",
    "dfPartial = dfFresh.append(dfRotten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4sqpQ3NGW4Z"
   },
   "source": [
    "La troisième étape consiste à créer un ensemble d'entraînement et un ensemble de test (validation) avec nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccmIoE1m-Nk6"
   },
   "outputs": [],
   "source": [
    "# Split the data such that 90% is used for training and 10% is used for testing (separating the review\n",
    "# from the freshness scores that we will use as the labels)\n",
    "# Recall that we do not use this test set when building the model, only the training set\n",
    "# We use the parameter stratify to split the training and testing data equally to create\n",
    "# a balanced dataset\n",
    "train_reviews, test_reviews, train_tags, test_tags = train_test_split(dfPartial[\"Review\"],\n",
    "                                                                      dfPartial[\"Freshness\"],\n",
    "                                                                      test_size=0.1, \n",
    "                                                                      random_state=3,\n",
    "                                                                      stratify=dfPartial[\"Freshness\"])\n",
    "train_tags = train_tags.to_numpy()\n",
    "train_reviews = train_reviews.to_numpy()\n",
    "# Testing set (what we will use to test the trained model)\n",
    "test_tags = test_tags.to_numpy()\n",
    "test_reviews = test_reviews.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1Fe47-f-Nk7"
   },
   "source": [
    "**2. Utilisation des ressources disponibles**  \n",
    "\n",
    "À la question 1, on vous a demandé de mettre en évidence quelques adjectifs menant à fresh ou rotten. Nous pouvons considérer ces mots comme des mots positifs ou négatifs. Il serait long de trouver nous-mêmes tous les mots positifs ou négatifs possibles, il est donc bon de rechercher de telles ressources.\n",
    "\n",
    "En fait, le champ de recherche de détection de la polarité est tellement populaire, que pour leur recherche, certains chercheurs ont établi des listes de mots positifs et négatifs.  Ceux utilisés dans ce notebook ont été téléchargés à partir d' [ici](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html) (un site Web sur Opinion Mining par le célèbre chercheur Bing Liu) et stockés localement.  Les fichiers *positive-words.txt* et *negative-words.txt* se trouvent dans votre liste de contrôle du Module 4 dans Brightspace.  Assurez-vous de placer ces fichiers dans le même répertoire que votre Notebook si vous travaillez localement ou de les avoir téléchargés vers Colab si vous travaillez dans Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HnajRezsHhdq"
   },
   "outputs": [],
   "source": [
    "# If you use colab but do not mount a google drive, you must upload the resources\n",
    "# Import the positive words\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V1CM6Iu7-Nk8",
    "outputId": "262dfda9-a4e4-40e4-9791-d6758553a817"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a+', 'abound', 'abounds', 'abundance', 'abundant', 'accessable', 'accessible', 'acclaim', 'acclaimed', 'acclamation', 'accolade', 'accolades', 'accommodative', 'accomodative', 'accomplish', 'accomplished', 'accomplishment', 'accomplishments', 'accurate', 'accurately', 'achievable', 'achievement', 'achievements', 'achievible', 'acumen', 'adaptable', 'adaptive', 'adequate', 'adjustable', 'admirable', 'admirably', 'admiration', 'admire', 'admirer', 'admiring', 'admiringly', 'adorable', 'adore', 'adored', 'adorer', 'adoring', 'adoringly', 'adroit', 'adroitly', 'adulate', 'adulation', 'adulatory', 'advanced', 'advantage', 'advantageous']\n"
     ]
    }
   ],
   "source": [
    "# Read the positive words\n",
    "\n",
    "with open(\"positive-words.txt\", encoding = \"ISO-8859-1\") as f:\n",
    "    posWords = f.readlines()\n",
    "posWords = [p[0:len(p)-1] for p in posWords if p[0].isalpha()] \n",
    "\n",
    "# print the first 50 words\n",
    "print(posWords[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0tnA76Se-Nk9",
    "outputId": "fdba968b-e668-4999-d0f4-3f79ec4e1be6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormal', 'abolish', 'abominable', 'abominably', 'abominate', 'abomination', 'abort', 'aborted', 'aborts', 'abrade', 'abrasive', 'abrupt', 'abruptly', 'abscond', 'absence', 'absent-minded', 'absentee', 'absurd', 'absurdity', 'absurdly', 'absurdness', 'abuse', 'abused', 'abuses', 'abusive', 'abysmal', 'abysmally', 'abyss', 'accidental', 'accost', 'accursed', 'accusation', 'accusations', 'accuse', 'accuses', 'accusing', 'accusingly', 'acerbate', 'acerbic', 'acerbically', 'ache', 'ached', 'aches', 'achey', 'aching', 'acrid', 'acridly', 'acridness', 'acrimonious', 'acrimoniously']\n"
     ]
    }
   ],
   "source": [
    "# Read the negative words\n",
    "\n",
    "with open(\"negative-words.txt\", encoding = \"ISO-8859-1\") as f:\n",
    "    negWords = f.readlines()\n",
    "negWords = [p[0:len(p)-1] for p in negWords if p[0].isalpha()] \n",
    "\n",
    "# Print the first 50 negative words\n",
    "print(negWords[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xn1FUuUzII3W"
   },
   "source": [
    "**(TO DO) Q2 - 3 points**\n",
    "\n",
    "Que pensez-vous de ces mots positifs et négatifs ? S'il y avait une troisième catégorie \"neutre\", déplaceriez-vous certains de ces mots dans la catégorie \"neutre\". Mentionnez-en quelques-uns et pourquoi. Vous pouvez modifier le code ci-dessus pour imprimer plus de 50 mots si vous n'en voyez aucun que vous modifieriez dans les 50 premiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gq58CeoIaGo"
   },
   "source": [
    "**RÉPONSE - Q2**\\\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoxR-dZ3-Nk-"
   },
   "source": [
    "**3. Approche de base**  \n",
    "\n",
    "Avant d’évaluer les performances d’une approche d’apprentissage supervisée, nous pouvons commencer par établir une approche de base très simple.  Il est toujours bon de commencer simple.  Une approche de base nous permet de mesurer si la complexité supplémentaire des différents modèles que nous développons en vaut la peine ou non.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCOHanelIt6M"
   },
   "source": [
    "***3.1 Algorithme***  \n",
    "\n",
    "L’ *algorithme de base* que nous allons utiliser compte simplement le nombre de mots positifs et négatifs dans la revue (critique) et donne la catégorie correspondant au plus grand nombre de mots (ce sera positif s'il y a plus de mots positifs et négatif dans le cas où il y a plus de mots négatifs).  Cette approche n’apprend rien.  Elle utilise simplement un *raisonnement* particulier (stratégie au moment du test).  Vous pourriez être surpris de savoir combien de *start-ups d'IA* dans le domaine d’Opinion Mining, utilisent ce genre d’approche simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJupuKCp-Nk-"
   },
   "outputs": [],
   "source": [
    "# First let's define methods to count positive and negative words\n",
    "\n",
    "def countPos(text):\n",
    "    count = 0\n",
    "    for t in text.split():\n",
    "        if t in posWords:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def countNeg(text):\n",
    "    count = 0\n",
    "    for t in text.split():\n",
    "        if t in negWords:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWNLI-R8-Nk_"
   },
   "outputs": [],
   "source": [
    "# Simple counting algorithm as baseline approach to polarity detection\n",
    "def baselinePolarity(review):\n",
    "    numPos = countPos(review)\n",
    "    numNeg = countNeg(review)\n",
    "    if numPos > numNeg:\n",
    "        return \"fresh\"   \n",
    "    else:\n",
    "        return \"rotten\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nirkQ59d-NlA"
   },
   "outputs": [],
   "source": [
    "# Test the baseline method\n",
    "print(\"Testing baselinePolarity with the review:\", train_reviews[0])\n",
    "print(\"baselinePolarity result:\", baselinePolarity(train_reviews[0]))\n",
    "print(\"Actual result:\", train_tags[0])\n",
    "print(\" \")\n",
    "print(\"Testing baselinePolarity with the review:\", train_reviews[1])\n",
    "print(\"baselinePolarity result:\", baselinePolarity(train_reviews[1]))\n",
    "print(\"Actual result:\", train_tags[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjgBeTwoJors"
   },
   "source": [
    "***3.2 Évaluation qualitative***\n",
    "\n",
    "Dans une évaluation qualitative, nous prenons un petit nombre de résultats et les étudions. Pour ce faire, nous devons prendre quelques résultats (test_reviews 0 à 5) et pour chacun regarder ce qui a fait dire à l'algorithme \"fresh\" ou \"rotten\" (sur quoi a-t-il basé sa décision).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvnoIR-tJpTv"
   },
   "source": [
    "**(TO DO) Q3 - 2 points**\n",
    "\n",
    "(a) Définissez les méthodes printPos et printNeg qui imprimeront tous les mots positifs et les mots négatifs trouvés dans un texte. \\\n",
    "(b) Après avoir testé 5 reviews (le code est déjà fourni), discutez des résultats. Pensez-vous que les mots trouvés sont des bons indicateurs de polarité? Pensez-vous que le résultat obtenu par l'algorithme est correct et que c'est plutôt celui attendu ne l'est pas ?  Souvenez-vous dans la vidéo sur l'évaluation que nous avons parlé de subjectivité... parfois nous ne sommes pas d'accord avec le \"gold standard\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCLHGXZcKiHu"
   },
   "outputs": [],
   "source": [
    "# ANSWER Q3 - Part a\n",
    "# def printPos(text):\n",
    "    ...\n",
    "\n",
    "# def printNeg(text):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MiODdVG9KlYj"
   },
   "outputs": [],
   "source": [
    "# Showing the review, the result and the positive and negative words contained\n",
    "for i in range(5):\n",
    "  print(test_reviews[i])\n",
    "  print(\"baselinePolarity result:\", baselinePolarity(test_reviews[i]))\n",
    "  print(\"Expected result:\", test_tags[0])\n",
    "  print(\"Positives words found: \")\n",
    "  printPos(test_reviews[i])\n",
    "  print(\"Negatives words found: \")\n",
    "  printNeg(test_reviews[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aM7QDnZ1KzoM"
   },
   "source": [
    "**RÉPONSE Q3 - Partie (b)**\\\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTUkevbF-NlA"
   },
   "source": [
    "***3.2 Évaluation quantitative***  \n",
    "\n",
    "Dans une évaluation quantitative, nous nous intéressons à des résultats exprimés avec des nombres, et pour que ces résultats soient significatifs, nous avons besoin d'un ensemble de test suffisamment grand. Bien que l'évaluation qualitative soit un bon outil d'analyse, l'évaluation quantitative est un bon outil comparatif permettant la comparaison de divers algorithmes.\n",
    "\n",
    "Pour tester notre *algorithme de base* nous utiliserons l’ensemble de test, défini plus tôt et calculerons le nombre de prédictions correctes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYy5Kkmk-NlC"
   },
   "outputs": [],
   "source": [
    "# Function takes a one dimensional array of reviews and a one dimensional array of\n",
    "# tags as input and prints the number of correct assignments when running the baseline approach\n",
    "# on the reviews.\n",
    "# Let's establish the polarity for each review\n",
    "def correctReviews(reviews, tags):\n",
    "    nbCorrect = 0\n",
    "    count = 0\n",
    "    for i in range(len(reviews)):\n",
    "        polarity = baselinePolarity(reviews[i])\n",
    "        if (count < 10):\n",
    "            print(reviews[i] + \" -- Prediction: \" + polarity + \". Actually: \" + tags[i] + \" \\n\")\n",
    "            count += 1\n",
    "        if (polarity == tags[i]):\n",
    "            nbCorrect += 1\n",
    "\n",
    "    print('There are %s correct predictions out of %s total predictions' %(nbCorrect, len(tags)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gi0_Zpf9-NlD"
   },
   "outputs": [],
   "source": [
    "# This may take a minute to run\n",
    "correctReviews(test_reviews, test_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEfX8PGf-NlD"
   },
   "source": [
    "**(TO DO) Q4 - 3 points**\n",
    "\n",
    "Le résultat calculé dans la méthode \"correctReviews\" ne fournit pas de résultats pour les classes individuelles (fresh, rotten), nous ne savons donc pas vraiment si l'algorithme fonctionne bien ou non sur chaque classe. Faisons plutôt le calcul du ***Rappel*** de chaque classe.\n",
    "\n",
    "(a) Modifiez le code ci-dessus afin qu'il fournisse des résultats de rappel par classe. \\\n",
    "(b) Discutez si les résultats de rappel sont différents entre les classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIBqTJSdP3RF"
   },
   "outputs": [],
   "source": [
    "# ANSWER Q4 - Write code (you can use the last few lines to guide you.. but you can change them if you want)\n",
    "def recallPerClass(reviews, tags):\n",
    "    ...\n",
    "    ...\n",
    "\n",
    "    print('There are %s correct in Rotten class out of %s total predictions' %(nbCorrectRotten, nbTotalRotten))\n",
    "    print('Recall for Rotten is' ... )    \n",
    "    print('There are %s correct in Fresh class out of %s total predictions' %(nbCorrectFresh, nbTotalFresh))  \n",
    "    print('Recall for Fresh is' ... )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3XwuXE3QAeU"
   },
   "outputs": [],
   "source": [
    "# This may take a minute to run (depending on the size of your test set, and whether you are on colab or not)\n",
    "recallPerClass(test_reviews, test_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWmZLbfRQUbs"
   },
   "source": [
    "**RÉPONSE Q4 - Partie (b)**\\\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxLgZRKo-NlE"
   },
   "source": [
    "**4. Méthode d'apprentissage supervisé**\n",
    "\n",
    "Nous allons maintenant entraîner un modèle d’apprentissage supervisé pour la détection de la polarité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMPcPEC9-NlF"
   },
   "source": [
    "***4.1 Données d'entraînements***  \n",
    "\n",
    "Dans l’apprentissage supervisé, nous avons besoin de données d'entraînement. Ces données d'entraînement doivent être *différentes* mais *représentatives* des données de test éventuelles. Au début du notebook, nous avons défini les données d'entraînement et les données de test comme un sous-ensemble de l’ensemble de données (20K/100K selon votre environnement tirées de 480K revues). Nous l’avons fait en raison du temps de calcul. En réalité, nous aurions  voulu utiliser l’ensemble de données et nous assurer que nous avons entraîné notre modèle avec un ensemble d'entraînement assez grand. Cela permettra de nous assurer que nous avons suffisamment appris afin de bien effectuer nos prédictions sur de nouvelles données.\n",
    "\n",
    "Habituellement, un ensemble d'entraînement doit être aussi grand et varié que possible. Les ensembles d'entraînement sont très précieux, mais ils sont coûteux à obtenir, car ils nécessitent un marquage (annotation humaine: positif et négatif par exemple) pour les générer et les données elles-mêmes peuvent avoir besoin d’être \"nettoyées\". Encore une fois, l’ensemble d'entraînement sera utilisé pour entraîner le modèle et l’ensemble de tests est utilisé pour tester la performance du modèle entraîné sur de nouveaux exemples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bI1K1o_M-NlF"
   },
   "outputs": [],
   "source": [
    "# Looking at the shapes of the train and test datasets that we will be using\n",
    "print(train_reviews.shape)\n",
    "print(test_reviews.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DF_Y_0t-NlG"
   },
   "source": [
    "***4.2 Prétraitement des données d'entrées*** \n",
    "\n",
    "Ce package d’apprentissage automatique, *scikit-learn*, est quelque peu particulier dans la façon dont les données doivent être mises en forme pour être utilisées par les algorithmes d'entraînement. Donc, nous devons effectuer un certain prétraitement sur les phrases ci-dessus.  Heureusement *scikit-learn* fournit quelques fonctions prédéfinis pour faire du prétraitement de texte. \n",
    "\n",
    "Nous transformons facilement chaque phrase en une liste d’index en dictionnaire. Le dictionnaire est construit à partir des mots dans les phrases. Les clés du dictionnaire sont les mots, et la valeur est un index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDe1ZFQO-NlG"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# The CountVectorizer builds a dictionary of all words (count_vect.vocabulary_), \n",
    "# and generates a matrix (train_counts), to represent each sentence\n",
    "# as a set of indices into the dictionary. The words in the dictionary are the words found in train_reviews.\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "train_counts = count_vect.fit_transform(train_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_l9Cb2xr-NlH"
   },
   "source": [
    "Pour comprendre ce que fait le code ci-dessus, nous allons d’abord imprimer le vocabulaire recueilli à partir des phrases dans train_reviews.  Vous verrez un ensemble de mots et d'index. Par exemple (« avec » : 4809) signifierait que le mot « avec » est associé à l'index 4809."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oT0MlKVm-NlH"
   },
   "outputs": [],
   "source": [
    "# print the vocabulary (dictionary of words)\n",
    "print(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAQUlECE-NlI"
   },
   "source": [
    "Ensuite, imprimons le *train_counts*. Vous verrez une liste de tuples avec un nombre. Par exemple (0,8790) 1 signifierait que la phrase 0, contient le mot associé à l'index 8790 1 fois. Ou (0, 31422) 3 signifierait que la phrase 0 contient le mot associé à l'index 31422, 3 fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNnqYRWC-NlI"
   },
   "outputs": [],
   "source": [
    "# print the content of the training examples in terms of frequency of words (each word represented by its index)\n",
    "print(train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKo86WW8-NlJ"
   },
   "source": [
    "\n",
    "Donc le train_counts contient pour chaque phrase, le BOW (bag of words, sac de mots) associé mais dans la forme d'une liste d'index (chaque index correspond à un mot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvScgXHL-NlJ"
   },
   "source": [
    "***4.3 Apprentissage Naive Bayes***\n",
    "\n",
    "Avec les données prétraitées, nous sommes prêts à tester l’algorithme Naive Bayes fourni par scikit-learn.  Cet algorithme exigeait que les données d'entraînement soient représentées en termes de *train counts* et c’est pourquoi nous avons fait le prétraitement ci-dessus.\n",
    "\n",
    "Étant donné que scikit-learn contient une méthode correspondant au classifeur Naive Bayes, il sera facile de l'utiliser.  Il est aussi facile d’effectuer l'*entraînement (fit)*, comme vous le voyez ci-dessous, pour entraîner le modèle.  Mais vous savez ou devez savoir ce qui s'y passe!!!  Il crée des probabilités antérieures pour les classes (fresh, rotten) et les probabilités postérieures de mots (caractéristiques) par classe (p. ex. P(terrible|fresh) (P(awful|fresh)) ou P(terrible|rotten) (P(awful|rotten)). Toutes ces probabilités sont utilisées dans le théorème de Bayes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ln_rkEAO-NlL"
   },
   "outputs": [],
   "source": [
    "# Test of a naive bayes algorithm, the \"fit\" is the training\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Training the model\n",
    "clf = MultinomialNB().fit(train_counts, train_tags)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSO1cwzu-NlL"
   },
   "source": [
    "***4.4 Evaluation de Naive Bayes***\n",
    "\n",
    "Examinons d’abord la performance du modèle sur l’ensemble d’entraînement.  Pour appliquer le modèle de classification (prédiction), nous utilisons la méthode *predict* ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CcgNWc--NlM"
   },
   "outputs": [],
   "source": [
    "# Testing on training set\n",
    "predicted = clf.predict(train_counts)\n",
    "# Print the first ten predictions\n",
    "for doc, category in zip(train_reviews[:10], predicted[:10]):   # zip allows to go through two lists simultaneously\n",
    "    print('%r => %s\\n' % (doc, category))\n",
    "correct = 0\n",
    "for tag, pred in zip(train_tags, predicted):   # zip allows to go through two lists simultaneously\n",
    "    if (tag == pred):\n",
    "        correct += 1\n",
    "print(\"Correctly classified %s total training examples out of %s examples\" %(correct, train_tags.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0pCyuYV-NlM"
   },
   "source": [
    "Habituellement, sur l’ensemble d'entraînement, nous obtenons la bonne prédiction sur la plupart des exemples.... C'est un résultat beaucoup trop optimiste. Mais nous devrions tester sur un **ensemble de test** réel, à savoir test_reviews et test_tags.\n",
    "\n",
    "**(TO DO) Q5 - 4 points**  \n",
    "\n",
    "Testez le modèle entraîné sur l'ensemble d'entraînement avec l'ensemble de test. (attention, ne réentrainez pas sur l'ensemble test)\\\n",
    "(a) Écrivez le code ci-dessous pour faire ce test. Avant le test, chaque ensemble de test doit être transformé avec les étapes de prétraitement (comme nous l'avons fait pour l'ensemble d'apprentissage auparavant), afin que leur format soit compatible avec l'apprenant. \\\n",
    "(b) Discutez si les résultats sont meilleurs sur l'ensemble d'apprentissage ou l'ensemble de test. Quelle est la différence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DOYz9-SgSnwH"
   },
   "outputs": [],
   "source": [
    "# RÉPONSE Q5 - Partie (a)\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpfjHTF-Sraz"
   },
   "source": [
    "**RÉPONSE Q5 - Partie (b)**\\\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiVDgyDV-NlN"
   },
   "source": [
    "***4.5 Évaluation:  Rappel, précision et F-mesure***\\\n",
    "Nous explorons les deux mesures d'évaluation présentées dans les vidéos : le rappel et la précision. Et nous introduisons une nouvelle mesure, le F-mesure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeFPMaqg-NlN"
   },
   "source": [
    "**(TO DO) Q6 - 2 points**   \n",
    "\n",
    "Une mesure d’**évaluation commune** dans l’apprentissage automatique est le **Rappel**. Vous avez déjà calculé le rappel de l'algorithme de base à la Question 4.\n",
    "\n",
    "Le Rappel est le nombre de prédictions correctes pour une classe d’intérêt (appelée les vrais positifs) divisé par le nombre total d’instances qui sont effectivement étiquetées comme étant dans cette classe d’intérêt (vrais positifs + faux négatifs).  Par exemple, si l’ensemble de test contient 5 exemples *Fresh* et que l’algorithme n’en a trouvé que 2, le rappel pour la classe *Fresh* est de 2/5.  Écrivez une petite méthode ci-dessous qui calculera le rappel d’une classe.  Il recevra trois paramètres :\n",
    "1. L'ensemble d'étiquettage correct (Ex: (fresh, rotten, fresh)), \n",
    "2. Les prédictions (Ex: (fresh, fresh, rotten)), et\n",
    "3. La classe d'intérêt (Ex: fresh).  \n",
    "\n",
    "La méthode doit retourner le *Rappel* (Recall) de la classe (Ex: 50%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lp7l5ghs-NlN"
   },
   "outputs": [],
   "source": [
    "# RÉPONSE Q6 - \n",
    "# Can be implemented in any way as long as it works correctly\n",
    "def recall(actualTags, predictions, classOfInterest):\n",
    "  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPkzOMWLT70h"
   },
   "source": [
    "Vous pouvez regarder les résultats de votre méthode avec le test ci-bas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTgMjbZ6T8Bs"
   },
   "outputs": [],
   "source": [
    "# Need to get the recall for the test set predictions from Naive Bayes\n",
    "print(recall(test_tags, test_predicted, \"fresh\"))\n",
    "print(recall(test_tags, test_predicted, \"rotten\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyZCsGgi-NlP"
   },
   "source": [
    "**(TO DO) Q7 - 2 points**   \n",
    "\n",
    "Une autre mesure d’**évaluation commune** dans l’apprentissage automatique s’appelle **Precision**. La précision est le nombre de prédictions correctes pour une classe d’intérêt (True Positives ou Vrais Positifs) divisé par le nombre total de fois où cette classe d’intérêt a été prédite (True Positives (Vrais Positifs) + False Positives (Faux Positifs)). Par exemple, si l’ensemble de test contient 3 exemples (1 à 3) *Fresh* et 1 exemple (4) *Rotten* et que l’algorithme a étiqueté les exemples (1, 2 et 4) comme *Fresh* et l'exemple (3) comme *Rotten*, alors la précision pour la classe *Fresh* est 2/3.  Écrivez une petite méthode ci-dessous qui calculera la précision d’une classe.  Il recevra trois paramètres : \n",
    "\n",
    "\n",
    "1. L'ensemble d'étiquettage correct (Ex: (fresh, rotten, fresh)), \n",
    "2. Les prédictions (Ex: (fresh, fresh, rotten)), et\n",
    "3. La classe d'intérêt (Ex: fresh).  \n",
    "\n",
    "La méthode doit retourner la *Précision* de la classe (Ex: 50%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eC9XW2rW-NlP"
   },
   "outputs": [],
   "source": [
    "# ANSWER Q7\n",
    "def precision(actualTags, predictions, classOfInterest):\n",
    "  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fV7n9uOfUmk1"
   },
   "source": [
    "Vous pouvez regarder les résultats de votre méthode avec le test ci-bas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xMb1s8yUn05"
   },
   "outputs": [],
   "source": [
    "# Get the precision for the test set predictions from Naive Bayes\n",
    "print(precision(test_tags, test_predicted, \"fresh\"))\n",
    "print(precision(test_tags, test_predicted, \"rotten\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zP0fahYf-NlP"
   },
   "source": [
    "**(TO DO) Q8 - 2 points**\n",
    "\n",
    "Une autre mesure d'évaluation populaire en apprentissage automatique est la F-mesure.\n",
    "\n",
    "F-Mesure fournit un moyen de combiner à la fois la précision et le rappel en une seule mesure qui capte les deux propriétés. Une fois que la précision et le rappel ont été calculés pour un problème de classification binaire ou multiclasses, les deux scores peuvent être combinés dans le calcul de la F-Mesure.\n",
    "\n",
    "La mesure F traditionnelle est calculée comme suit : F-Mesure = (2 * Précision * Rappel) / (Précision + Rappel) C'est la moyenne harmonique des deux fractions. Ceci est parfois appelé le F-Score ou le F1-Score et pourrait être la mesure la plus couramment utilisée sur les problèmes de classification déséquilibrée.\n",
    "\n",
    "Vous avez déjà implémenté des fonctions de précision et de rappel. Réutilisez maintenant ces deux valeurs pour implémenter la fonction F-mesure pour les deux classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRaN0dRB-NlQ"
   },
   "outputs": [],
   "source": [
    "# ANSWER Q8\n",
    "def fmeasure(precision_score, recall_score):\n",
    "  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTcaXh0lVmNl"
   },
   "source": [
    "Vous pouvez regarder les résultats de votre méthode avec le test ci-bas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlorM0YoVnnX"
   },
   "outputs": [],
   "source": [
    "precision_score = precision(test_tags, test_predicted, \"fresh\")\n",
    "recall_score = recall(test_tags, test_predicted, \"fresh\")\n",
    "print (fmeasure(precision_score, recall_score))\n",
    "precision_score = precision(test_tags, test_predicted, \"rotten\")\n",
    "recall_score = recall(test_tags, test_predicted, \"rotten\")\n",
    "print (fmeasure(precision_score, recall_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PttbnL_WvQb"
   },
   "source": [
    "**(TO DO) Q9 - 2 points**\n",
    "\n",
    "Maintenant que vous avez les résultats pour la précision, le rappel et la F-Mesure, analysez ces résultats. Voici trois questions pour vous aider dans votre analyse:\n",
    "\n",
    "\n",
    "*   Est-ce qu'il y a une grande différence entre la classe Fresh et Rotten pour les divers résultats?  Qu'est-ce qui pourrait expliquer cela?\n",
    "*   Est-ce qu'il y a une grande différence entre le rappel et la précision?  Qu'est-ce qui pourrait expliquer cela?\n",
    "*   Trouvez-vous que les résultats sont suffisamment bons pour utiliser cet algorithme?  Si oui, pourquoi, si non pourquoi?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXWU4ViyXegb"
   },
   "source": [
    "**RÉPONSE Q9 -** \\\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaELUms6WPFl"
   },
   "source": [
    "***4.6 Comparaison à l'algorithme de base***\n",
    "\n",
    "Maintenant que nous avons testé l'algorithme Naive Bayes, nous pouvons évaluer sa valeur en le comparant à notre approche de base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vGeHtkvYQpe"
   },
   "source": [
    "**(TO DO) Q10 - 3 points**\n",
    "\n",
    "L'approche Naive Bayes est-elle plus performante que l'approche de base, si oui de combien ? Vous avez programmé uniquement le rappel (pas la précision) pour l'approche de base, vous pouvez donc exprimer votre comparaison en utilisant uniquement le rappel.  Indiquez combien de données d'entraînement et de test ont été utilisées.  Dites si vous trouvez les gains importants ou non."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kG0IW-UYlz4"
   },
   "source": [
    "**RÉPONSE - Q10**\n",
    "\n",
    "En utilisant X données d'entraînement, et Y données de test.\n",
    "\n",
    "Rappel pour le baseline sur Fresh: \\\n",
    "Rappel pour l'algorithme Naive Bayes sur Fresh :  \\\n",
    "\n",
    "Rappel pour le baseline sur Rotten: \\\n",
    "Rappel pour l'algorithme Naive Bayes sur Rotten :  \\\n",
    "\n",
    "Les gains sont de ... \\\n",
    "\n",
    "Les gains sont importants? négligeables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeNLiEx7-NlU"
   },
   "source": [
    "***SIGNATURE:***\n",
    "Mon nom est Mark-Olivier Poulin.\n",
    "Mon numéro d'étudiant(e) est 300058025.\n",
    "Je certifie être l'auteur(e) de ce devoir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CSI4506_ApprentissageSupervise_Automne2021_Revised.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
