{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsCQnroFWbpA"
   },
   "source": [
    "# Notebook 10 - Apprentissage par renforcement / Taxi autonome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jS9f1M7hWblx"
   },
   "source": [
    "CSI4106 Intelligence Artificielle   \n",
    "Automne 2021  \n",
    "Version 1 (2020) preparée par Julian Templeton, Caroline Barrière et Joel Muteba.  Version 2 (2021) adaptée par Caroline Barrière."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWknU1E2Wbi8"
   },
   "source": [
    "***INTRODUCTION***:  \n",
    "Dans ce notebook, nous explorerons l'utilisation de l'apprentissage par renforcement pour aider un agent à résoudre une tâche spécifique dans un environnement fourni par la [bibliothèque OpenAI's Gym](https://gym.openai.com/). \n",
    "\n",
    "OpenAI's Gym propose de nombreuses expérimentations différentes à utiliser. Celles-ci vont d'exercices d'équilibre d'un bâton aux voitures autonomes en passant par le jeu simple d'Atari. Malheureusement, toutes les expérimentations disponibles ne peuvent pas être facilement utilisées. Beaucoup peuvent prendre des heures d'entraînement pour commencer à voir des résultats passionnants. Chacune de ces expérimentations utilise des agents qui peuvent être entraînés par apprentissage par renforcement pour maîtriser comment exécuter la tâche spécifiée. Les méthodes utilisées peuvent aller de la simple utilisation du Q-Learning à l'utilisation plus complexe d'un ou plusieurs modèles de Deep Learning qui fonctionnent en conjonction avec des techniques d'apprentissage par renforcement.\n",
    "\n",
    "Dans ce notebook, nous explorerons un scénario dans lequel un taxi autonome situé dans une grille doit prendre un passager situé dans l'une de quatre positions et déposer le passager dans l'une de trois autres positions.\n",
    "\n",
    "Pour vous familiariser avec le problème du **taxi autonome** abordé dans ce notebook, veuillez vous rendre sur le site https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/ et lire: \\\n",
    "\n",
    "\n",
    "*   section 1 (récompenses)\n",
    "*   section 2 (espace d'états) qui vous fera comprendre pourquoi il y a 500 états possibles, \n",
    "*   section 3 (espace d'action) qui décrit les actions possibles.\n",
    "\n",
    "Tout au long du notebook, nous travaillerons avec une approche de base aléatoire et une approche basée sur le Q-Learning. Cela fournira un aperçu de la façon dont Q-Learning peut être appliqué aux problèmes et comment un agent peut utiliser l'apprentissage par renforcement pour résoudre des problèmes dans un environnement.  La comparaison avec l'approche aléatoire montrera le potentiel du Q-Learning.\n",
    "\n",
    "**Lors de la soumission de ce notebook, assurez-vous de NE PAS réinitialiser les sorties de l'exécution du code (et n'oubliez pas d'enregistrer le notebook avec ctrl + s).**\n",
    "\n",
    "**Afin de faciliter l'installation, vous exécuterez à nouveau ce notebook dans Google Colab, PAS sur votre ordinateur local.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iCMGO9VWbgM"
   },
   "source": [
    "***DEVOIR***:  \n",
    "Parcourez le notebook en exécutant chaque cellule, une à la fois.\n",
    "Recherchez **(TO DO)** pour les tâches que vous devez effectuer. Ne modifiez pas le code en dehors des questions auxquelles vous êtes invité à répondre à moins que cela ne vous soit spécifiquement demandé. Une fois que vous avez terminé, signez le notebook (à la fin du notebook), renommez-le *NumEtudiant-NomFamille-Notebook10.ipynb* et soumettez-le.\n",
    "\n",
    "*Le notebook sera noté sur 30.  \\\n",
    "Chaque **(TO DO)** a un certain nombre de points qui lui sont associés.*\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIhGNTB-Wbbs"
   },
   "source": [
    "**1.0 - Mise en place du jeu de taxi**   \n",
    "\n",
    "Pour commencer le notebook, nous devrons configurer et explorer l'environnement de Open Gym pour le taxi autonome.\n",
    "\n",
    "Le taxi autonome doit prendre et déposer un passager dans un espace défini par une petite grille. Pour bien saisir l'environnement dans lequel le taxi autonome (l'agent) évolue, assurez-vous de lire les 3 sections du tutoriel sur les récompenses, les états et les actions, tel que mentionné dans l'introduction de ce notebook.\n",
    "\n",
    "Le code utilisé dans tout le notebook provient de [cet exemple](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/) et a été modifié conséquemment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEQK-Xep0MrF"
   },
   "source": [
    "Pour commencer, nous installerons certains des packages dont nous aurons besoin pour exécuter le programme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPYIG4d6Ztzk"
   },
   "outputs": [],
   "source": [
    "# Install the necessary libraries\n",
    "!pip install cmake 'gym[atari]' scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O0vOVuqHA8SM"
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3nhPCUHb2cO"
   },
   "source": [
    "Avec toutes les bibliothèques installées, nous allons maintenant utiliser le programme Taxi fourni par Gym. Ci-dessous, nous allons importer Gym, charger le programme comme environnement actif et montrer une image représentant l'état actuel du programme.\n",
    "\n",
    "D'après l'image ci-dessous, il existe quatre emplacements clés différents dans l'environnement, représentés par *R*, *G*, *Y* et *B*. La lettre qui est en gras en bleu représente l'endroit où le passager actuel doit être récupéré et la lettre en gras en violet représente l'endroit où le passager souhaite être déposé. Le bloc jaune représente la cellule dans laquelle se trouve actuellement le taxi. Par conséquent, le taxi doit d'abord récupérer le passager et le déposer au lieu désiré. Lorsqu'un passager est dans le taxi, la cellule jaune devient verte jusqu'à ce que le passager soit déposé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2iMY2MbaFSk"
   },
   "outputs": [],
   "source": [
    "# Load the environment\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "# Render the current state of the program\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oq2BvrwdPh-"
   },
   "source": [
    "Ensuite, nous allons réinitialiser l'état de l'environnement et montrer à nouveau l'état actuel. Nous imprimons également le nombre total d'actions disponibles pour notre agent (défini comme *l'espace d'action*) et *l'espace d'état* qui représente l'état du programme (où se trouve la cabine, le passager, le lieu de prise en charge et le lieu où déposer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlLevJ7KaHYc"
   },
   "outputs": [],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGp8quCqeGjU"
   },
   "source": [
    "Nous voulons que notre agent apprenne quelle action entreprendre dans un état spécifique. Cet état dépend de l'emplacement du taxi par rapport à l'emplacement du passager et au lieu où déposer. Les six actions possibles que le taxi peut effectuer dans un état donné sont:\n",
    "\n",
    "Action = 0: Aller vers le Sud\n",
    "\n",
    "Action = 1: Aller vers le Nord\n",
    "\n",
    "Action = 2: Aller vers l'Est\n",
    "\n",
    "Action = 3: Aller vers l'Ouest\n",
    "\n",
    "Action = 4: Prendre le passager\n",
    "\n",
    "Action = 5: Déposer le passager\n",
    "\n",
    "Vous trouverez ci-dessous un exemple de définition de l'état avec un codage spécifique et le rendu de cet état."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7QM71s0aded"
   },
   "outputs": [],
   "source": [
    "# The encoding below represents: (taxi row, taxi column, passenger index, destination index)\n",
    "state = env.encode(3, 1, 2, 0) \n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCSHJMfb2Hll"
   },
   "source": [
    "L'exemple suivant montre comment définir un état avec le passager dans le taxi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPshGWDomKfc"
   },
   "outputs": [],
   "source": [
    "# The encoding below represents: (taxi row, taxi column, passenger index, destination index)\n",
    "state = env.encode(0, 1, 4, 0) \n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3b__QWRTRB3"
   },
   "source": [
    "**(TO DO) Q1 - 4 points**   \n",
    "Maintenant que nous avons vu comment définir un état via un encodage, vous devrez définir l'état pour qu'il corresponde aux descriptions ci-dessous et les montrer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YKbEIP2WAkb"
   },
   "source": [
    "**(TO DO) Q1 (a) - 2 points**    \n",
    "Mettez le passager à la position G, le passager souhaitant être déposé à la position R.  Le taxi doit être positionné à un point aléatoire sur la grille (la position sélectionnée du taxi doit être choisie au hasard). Après avoir défini la position, montrez le rendu (avec render) de l'état."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yT05ErEOTRPl"
   },
   "outputs": [],
   "source": [
    "# RÉPONSE Q1(a) \n",
    "# remember to use random coordinates within the grid for the taxi...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pl8bVtwWIdE"
   },
   "source": [
    "**(TO DO) Q1 (b) - 2 points**\n",
    "\n",
    "Mettez le passager pour qu'il soit dans le taxi (à n'importe quelle position sans lettre dessus) et mettez le point de débarquement des passagers sur la position B. Après avoir défini la position, montrer le rendu (avec render) l'état."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6roy4E4NWIv0"
   },
   "outputs": [],
   "source": [
    "# RÉPONSE Q1(b)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTyOS_Hbi3Rf"
   },
   "source": [
    "Pour chaque action que le taxi peut entreprendre, nous avons une liste représentant les informations clés par rapport à ce qui se passera lorsqu'une action est effectuée. Après avoir effectué une action, l'agent recevra une récompense ou une pénalité et l'environnement se retrouvera dans un nouvel état.\n",
    "\n",
    "Ci-dessous, nous affichons un dictionnaire (similaire à la table de récompense - reward table - présentée dans la vidéo du cours) qui contient toutes les actions possibles ainsi que les informations suivantes dans les tuples correspondants:\n",
    "\n",
    "(\n",
    "\n",
    "   La probabilité de prendre cette action,\n",
    "  \n",
    "   L'état résultant après avoir effectué cette action,\n",
    "  \n",
    "   La récompense pour avoir pris cette action,\n",
    "  \n",
    "   Si le programme se terminera ou non lors de l'exécution de l'action\n",
    "\n",
    ")\n",
    "\n",
    "Exemple de tuple: (1.0, 328, -1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6Ga3Y6yagfE"
   },
   "outputs": [],
   "source": [
    "env.P[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_57Z2adUzAJH"
   },
   "source": [
    "Bien que non affiché par le code ci-dessus, si le taxi contient le passager et est au-dessus du point de débarquement, la récompense pour l'action de déposer le passager est de 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iiy-KNusvBDJ"
   },
   "source": [
    "**2.0 - Approche de base du jeu du taxi**   \n",
    "\n",
    "Pour commencer, nous allons effectuer la simulation du scénario de taxi avec une approche de base qui n'utilise pas Q-Learning. Cette approche fonctionnera simplement en sélectionnant une action aléatoire disponible à chaque pas de temps, quel que soit l'état actuel. Nous préparerons également une méthode de génération de toutes les images d'un épisode pour voir comment l'agent contrôle le taxi dans le scénario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKnHAA5SaitM"
   },
   "outputs": [],
   "source": [
    "def run_single_simulation_baseline(env, state, disable_prints=False):\n",
    "    '''\n",
    "    Given the environment and a specific state, randomly select an action for the taxi\n",
    "    to perform until the goal is completed.\n",
    "    '''\n",
    "    if not disable_prints:\n",
    "        print(\"Testing for simulation: {}\".format(state))\n",
    "    # Set the state of the environment\n",
    "    env.s = state\n",
    "    # Used to hold all information for a single time step (including the image data)\n",
    "    frames = []\n",
    "    # Used to determine when the simulation has been completed\n",
    "    done = False\n",
    "    # Determines the number of times steps that the application has been run for\n",
    "    time_steps = 0\n",
    "    # The total values used to determine how many times the agent mistakenly\n",
    "    # picks up no one or attempts to dropoff no passenger or attempts to\n",
    "    # dropoff a passenger in the wrong position.\n",
    "    penalties, reward = 0, 0\n",
    "    # Run until the passenger has been picked up and dropped off in the target location\n",
    "    while not done:\n",
    "        # Perform a random action from the set of available actions in the environment\n",
    "        action = env.action_space.sample()\n",
    "        # From performing the action, retrieve the new state, the reward from taking the action,\n",
    "        # whether the simulation is complete, and other information from performing the action.\n",
    "        state, reward, done, info = env.step(action)\n",
    "        # If an incorrect dropoff or pickup is performed, increment the penalty count\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "        # Put each rendered frame into dict to use for animating the process and\n",
    "        # tracking the details over the run\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "            }\n",
    "        )\n",
    "        # Increment the time step count\n",
    "        time_steps += 1\n",
    "    # State the total number of steps taken and the total penalties that have occured.\n",
    "    if not disable_prints:\n",
    "        print(\"Timesteps taken: {}\".format(time_steps))\n",
    "        print(\"Penalties incurred: {}\".format(penalties))\n",
    "    # Return the frame data, the total penalties, and the total time steps\n",
    "    return frames, penalties, time_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fk5lkc3Q5XLv"
   },
   "source": [
    "Une fois l'approche de base définie, nous allons exécuter un test avec cette approche pour voir combien de temps il faut à un agent utilisant cette approche pour trouver une solution pour la simulation 328 et combien de pénalités majeures l'agent reçoit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTLKSCRq4bEG"
   },
   "outputs": [],
   "source": [
    "state = 328\n",
    "# Run a test and collect all frames from the run\n",
    "frames, _, _ = run_single_simulation_baseline(env, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayN1up479Fl1"
   },
   "source": [
    "Après avoir effectué une simulation et récupéré les résultats, nous pouvons utiliser les images obtenues à partir de la simulation et les transmettre à la fonction *print_frames* ci-dessous pour afficher une animation contenant toutes les images ainsi que les informations qui provenaient de chaque étape et temps auquel l'image correspond .\n",
    "\n",
    "Pour le premier épisode que vous visualisez, il est recommandé d'exécuter l'ensemble du processus à une vitesse plus lente (telle que 0.3 ou 0.5 pour le paramètre sleepTime). Cependant, vous êtes libre d'augmenter la vitesse du processus par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bt-sMubValJt"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames, sleepTime=0.3):\n",
    "    '''\n",
    "    For each frame, show the frame and display the timestep it occurred at,\n",
    "    the number of the active state, the action selected, adn the corresponding reward.\n",
    "    '''\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        # Can adjust speed here\n",
    "        sleep(sleepTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rp2v5Q1x1rdF"
   },
   "outputs": [],
   "source": [
    "# Print the frames from the episode\n",
    "print_frames(frames, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRCoIPbi7OCv"
   },
   "source": [
    "**(TO DO) Q2 - 2 points**\n",
    "\n",
    "Regardons quelques simulations d'une stratégie aléatoire.  Pour ce faire, vous débuterez avec les états que vous avez établis pour Q1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-N9fP_Ydvot"
   },
   "source": [
    "**(TO DO) Q2 (a) - 1 point**   \n",
    "En utilisant l'état défini à partir de Q1 (a), récupérer les frames/images correspondantes obtenues en utilisant l'approche de base ci-dessus. Ensuite, affichez ces cadres(frames ou images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDCSqmbM-USp"
   },
   "outputs": [],
   "source": [
    "# RÉPONSE Q2(a)\n",
    "# Retrieve the corresponding frames from running the simulation starting from the state found in Q1 (a). Then show those frames.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3Dufixud28j"
   },
   "source": [
    "**(TO DO) Q2(b) - 1 point**\n",
    "\n",
    "a) En utilisant l'état défini à partir de Q1 (b), récupérer les frames/images correspondantes obtenues en utilisant l'approche de base ci-dessus. Ensuite, affichez ces cadres(frames ou images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMcngIsad3Mu"
   },
   "outputs": [],
   "source": [
    "# RÉPONSE Q2(b)\n",
    "# Retrieve the corresponding frames from running the simulation starting from the state found in Q1 (b). Then show those frames.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwgxcH_Ojfud"
   },
   "source": [
    "Avec la possibilité de simuler des exécutions uniques d'un épisode avec l'approche de base, nous allons maintenant définir une fonction que nous utiliserons pour évaluer les performances générales du modèle de base lorsqu'il est exécuté sur plusieurs épisodes. La fonction *evaluate_agent_baseline* ci-dessous accepte comme entrée le nombre total d'épisodes sélectionnés au hasard à exécuter avec l'environnement, exécute les épisodes aléatoires, affiche le nombre moyen de pas de temps pris par épisode avec les pénalités moyennes encourues et renvoie les données d'image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7GLAT5_mjgEu"
   },
   "outputs": [],
   "source": [
    "def evaluate_agent_baseline(episodes, env):\n",
    "    '''\n",
    "    Given a number of episodes and an environment, run the specified\n",
    "    number of episodes, where each run begins with a random state, display the\n",
    "    naverage timesteps per episode and the average penalties per episode, and output\n",
    "    the frames to be displayed.\n",
    "    '''\n",
    "    total_time_steps, total_penalties = 0, 0\n",
    "    frames = []\n",
    "    # Run through the total number of episodes\n",
    "    for _ in range(episodes):\n",
    "        # Get a random state\n",
    "        state = env.reset()\n",
    "        # Run the simulation, obtaining the results\n",
    "        frame_data, penalties, time_steps = run_single_simulation_baseline(env, state, True)\n",
    "        # Update the tracked data over all simulations\n",
    "        total_penalties += penalties\n",
    "        total_time_steps += time_steps\n",
    "        frames = frames + frame_data\n",
    "    print(f\"Results after {episodes} episodes:\")\n",
    "    print(f\"Average timesteps per episode: {total_time_steps / episodes}\")\n",
    "    print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHkepDd2C3im"
   },
   "source": [
    "**(TO DO) Q3 - 5 points**\n",
    "\n",
    "Évaluons la méthode de base aléatoire.  Pour ce faire, nous devons exécuter un certain nombre d'épisodes pour faire une moyenne des résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3Tnw6bBxMXv"
   },
   "source": [
    "**(TO DO) Q3(a) - 1 point**\n",
    "\n",
    "Utilisez la fonction *evaluate_agent_baseline* définie ci-dessus pour exécuter 100 épisodes aléatoires pour l'environnement.\n",
    "\n",
    "***Si la fonction evaluate_agent_baseline semble s'exécuter trop longtemps (plusieurs minutes, pas une seule), arrêtez l'exécution en cliquant sur le bouton en haut à gauche de la cellule de code en cours d'exécution et réexécutez-la.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPRemQq-xXUT"
   },
   "outputs": [],
   "source": [
    "# RÉPONSE Q3(a) \n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dClJX6AWxZM5"
   },
   "source": [
    "**(TO DO) Q3(b) - 2 points**\n",
    "\n",
    "D'après les résultats de la Q3 (a), comment l'approche de référence a-t-elle fonctionné et pourquoi pensez-vous qu'elle a bien ou mal fonctionné? Expliquez en référent aux temps moyens par épisode et aux pénalités moyennes par épisode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxUTqzCSxhZU"
   },
   "source": [
    "**RÉPONSE Q3(b)**   \n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Wr3F6IExrQQ"
   },
   "source": [
    "**(TO DO) Q3(c) - 2 points**\n",
    "\n",
    "Sans suggérer d'utiliser une approche d'apprentissage par renforcement (comme nous le ferons dans la partie 3), suggérez 2 idées qui pourraient être incluses dans une approche de base améliorée pour lui permettre de fonctionner légèrement mieux ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_b7h54SDQ7C"
   },
   "source": [
    "**RÉPONSE Q3(c)**\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GfrTVl3AuhD"
   },
   "source": [
    "**3.0 - Entraîner un agent avec Q-Learning pour jouer au jeu du taxi**   \n",
    "\n",
    "Maintenant que nous avons vu une simulation du taxi autonome utilisant une stratégie aléatoire, nous nous tournons vers l'apprentissage par renforcement pour obtenir une meilleure stratégie par l'intermédiaire de l'algorithme Q-Learning.\n",
    "\n",
    "Pour démarrer le processus, nous allons créer une table de valeurs Q pour chaque possibilité d'état d'action (en l'initialisant à zéro). L'agent mettra à jour cette table lors de l'entraînement et *ATTENTION*, il aura besoin de la ***réinitialiser*** chaque fois qu'il souhaite réinitialiser son entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDTbzsaEanoU"
   },
   "outputs": [],
   "source": [
    "# Initialize the table of Q values for the state-action pairs\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mph6_GdSGy2F"
   },
   "source": [
    "Une fois la Q-Table initialisée, nous allons maintenant définir la fonction d'apprentissage qui ajuste les valeurs Q dans *q_table*. Le processus d'apprentissage consiste à exécuter un certain nombre de simulations aléatoires et à mettre à jour les valeurs Q pour chaque état via Q-Learning.\n",
    "\n",
    "Il existe un certain nombre d'hyperparamètres utilisés par la fonction d'apprentissage:\n",
    "\n",
    "- *alpha*: paramètre d'apprentissage \n",
    "- *gamma*: le paramètre de *discount* sur le gain à long terme.\n",
    "- *epsilon*: paramètre d'Exploitation/Exploration\n",
    "- *num_simulations*: nombre d'épisodes à générer pour que le taxi autonome mette à jour ses valeurs Q.\n",
    "\n",
    "Ainsi, en exécutant cet algorithme, un agent peut apprendre les valeurs Q à utiliser lorsqu'il sera par la suite mis en situation de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kbLhT-Ua0Cc"
   },
   "outputs": [],
   "source": [
    "def train_agent(alpha, gamma, epsilon, num_simulations):\n",
    "    '''\n",
    "    Trains an agent by updating its Q values for a total of num_simulations\n",
    "    episodes with the alpha, gamma, and epsilon hyperparameters. \n",
    "    '''\n",
    "    # For plotting metrics\n",
    "    all_time_steps = []\n",
    "    all_penalties = []\n",
    "    # Generate the specified number of episodes\n",
    "    for i in range(1, num_simulations + 1):\n",
    "        # Generate a new state by resetting it\n",
    "        state = env.reset()\n",
    "        # Variables tracked (time steps, total penalties, the reward value)\n",
    "        time_steps, penalties, reward, = 0, 0, 0\n",
    "        done = False\n",
    "        # Run the simulation \n",
    "        while not done:\n",
    "            # Select a random action is the randomly selected number from a\n",
    "            # uniform distribution is less than epsilon\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample() # Explore action space\n",
    "            # Otherwise use the currently learned Q values\n",
    "            else:\n",
    "                action = np.argmax(q_table[state]) # Exploit learned values\n",
    "            # Retrieve the relevant information after performing the action\n",
    "            next_state, reward, done, info = env.step(action) \n",
    "            # Retrieve the old Q value and the maximum Q value from the next state\n",
    "            old_value = q_table[state, action]\n",
    "            next_max = np.max(q_table[next_state])\n",
    "            # Update the current Q value\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            q_table[state, action] = new_value\n",
    "            # Track anytime an incorrect dropoff or pickup is made\n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "            # Proceed to the next state and time step\n",
    "            state = next_state\n",
    "            time_steps += 1\n",
    "        # Display progress for each 100 episodes\n",
    "        if i % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode: {i}\")\n",
    "    print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLoNggkqQu2L"
   },
   "source": [
    "Nous utilisons maintenant la fonction d'entraînement avec un ensemble d'hyperparamètres pour entraîner l'agent avec Q-Learning afin d'améliorer potentiellement les performances au fil du temps.  Le nombre d'épisodes est établi à 100000, donc l'apprentissage va prendre un peu de temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tkJvaTwPjfB"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.5\n",
    "epsilon = 0.1\n",
    "num_simulations = 100000\n",
    "# Train the agent\n",
    "train_agent(alpha, gamma, epsilon, num_simulations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmzQceEvRyT3"
   },
   "source": [
    "Après l'entraînement, nous pouvons regarder les valeurs Q qui ont été obtenues dans notre table state-action pour un état spécifique. Ci-dessous, nous voyons que chaque valeur Q pour les six actions possibles disponibles pour l'état 328 a été mise à jour en conséquence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvXOzHtVAar6"
   },
   "source": [
    "**(TO DO) Q4 - 2 points**\n",
    "\n",
    "Ci-dessous, nous imprimons les valeurs Q disponibles pour les six actions à l'état 328 et nous montrons cet état pour le visualiser. Sur la base des valeurs Q disponibles (en supposant que nous sommes en mode d'exploitation), quelle action serait la prochaine à être sélectionnée (ou s'il y a des égalités, listez toutes les actions possibles qui seraient envisagées)? Certaines des actions contenant des valeurs Q plus élevées semblent-elles problématiques si jamais elles sont sélectionnées? Pourquoi ou pourquoi pas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-KP5FMBQa1el"
   },
   "outputs": [],
   "source": [
    "print(q_table[328])\n",
    "env.s = 328\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xx4axJNOBl_0"
   },
   "source": [
    "**RÉPONSE Q4**\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncwGTV5PSHZx"
   },
   "source": [
    "Une fois l'entraînement terminé, nous pouvons maintenant évaluer l'approche Q-Learning d'une façon similaire à l'évaluation de l'approche de base. En passant le nombre d'épisodes à tester et l'environnement, nous générons ce nombre d'épisodes aléatoires et faisons la moyenne des résultats obtenus en exécutant l'approche Q-Learning pour effectuer les épisodes. Contrairement à l'entraînement, il est important de noter que les hyperparamètres qui étaient utilisés ne sont plus utilisés ici. L'agent utilise simplement la valeur Q maximale à chaque étape pour déterminer quelle action entreprendre à un pas de temps donné.  Il s'agit en fait d'une stratégie gloutonne (et non epsilon-gloutonne) qui suit uniquement la valeur de Q maximale.  Nous testons donc en mode d'exploitation (et non exploration/exploitation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuitaI8Ra2u8"
   },
   "outputs": [],
   "source": [
    "def evaluate_agent_QL(episodes, env):\n",
    "    '''\n",
    "    Given a number to specify how many random states to run and the environment to use,\n",
    "    display the averaged metrics obtained from the tests and return the frames obtained from the tests.\n",
    "    '''\n",
    "    total_time_steps, total_penalties = 0, 0\n",
    "    frames = []\n",
    "    for _ in range(episodes):\n",
    "        # Generate a random state to use\n",
    "        state = env.reset()\n",
    "        # The information collected throughout the run\n",
    "        time_steps, penalties, reward = 0, 0, 0\n",
    "        # Determines when the episode is complete\n",
    "        done = False\n",
    "        # Run through the episode until complete\n",
    "        while not done:\n",
    "            # Select the action containing the maximum Q value\n",
    "            action = np.argmax(q_table[state])\n",
    "            # Run that action and retrieve the reward and other details\n",
    "            state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Put each rendered frame into dict for animation\n",
    "            frames.append({\n",
    "                'frame': env.render(mode='ansi'),\n",
    "                'state': state,\n",
    "                'action': action,\n",
    "                'reward': reward\n",
    "                }\n",
    "            )\n",
    "            # Specify whether the agent incorrectly chose to pick up or dropoff a passenger\n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "            # Increment the current time step\n",
    "            time_steps += 1\n",
    "        # Track the totals\n",
    "        total_penalties += penalties\n",
    "        total_time_steps += time_steps\n",
    "    # Display the performance over the tests\n",
    "    print(f\"Results after {episodes} episodes:\")\n",
    "    print(f\"Average timesteps per episode: {total_time_steps / episodes}\")\n",
    "    print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
    "    # Return the frames to allow a user to view the runs\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEQNpWoSF822"
   },
   "source": [
    "**(TO DO) Q5 - 3 points**\n",
    "\n",
    "Il y a un aspect aléatoire dans Q-learning, il faut donc courir un certain nombre d'épisodes pour donner un résultat moyen. Fixons ce nombre suffisamment haut pour que la moyenne soit significative. Voyons comment Q-learning se comporte par rapport à l'approche de base.\n",
    "\n",
    "***ATTENTION: Si la fonction evaluate_agent_QL semble s'exécuter trop longtemps (30s ou plus), arrêtez l'exécution en cliquant sur le bouton en haut à gauche de la cellule de code en cours d'exécution et réexécutez-la. Cela se produit parce que l'entraînement était suffisant pour définir des valeurs Q valides et a entraîné une impasse pour un état spécifique.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TzyRekwGolK"
   },
   "source": [
    "**(TO DO) Q5 (a) - 1 point**\n",
    "\n",
    "Exécutez *evaluate_agent_QL* pendant 500 épisodes pour récupérer le nombre moyen de pas de temps et la pénalité moyenne après l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAliG8sjTzBc"
   },
   "outputs": [],
   "source": [
    "# RÉPONSE Q5(a)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_Os_s3UGxK2"
   },
   "source": [
    "**(TO DO) Q5 (b) - 2 points**\n",
    "\n",
    "Compte tenu de vos résultats de Q5 (a), comment les résultats observés des tests se comparent-ils aux tests de l'approche de base de Q3 (a)? Plus précisément, quelle stratégie est la plus performante par rapport au nombre moyen de pénalités tout au long des tests et quelle stratégie est capable de mener les passagers du point de départ à la destination plus rapidement (en moyenne)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPjrUlM4G_f_"
   },
   "source": [
    "**RÉPONSE Q5(b)**\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woAhj7vqaD5q"
   },
   "source": [
    "**4.0 - Test de différents hyperparamètres**   \n",
    "\n",
    "Nous allons maintenant essayer de re-entraîner l'agent en utilisant différentes configurations avec les hyperparamètres. Cela vous permettra d'explorer leur impact sur le Q-Learning et de comprendre leur objectif pendant l'entraînement.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBamDNUIZ0Iy"
   },
   "source": [
    "**(TO DO) Q6 - 12 points**     \n",
    "Ci-dessous, nous explorons différentes variations de quatre hyperparamètres utilisés par l'approche Q-Learning pour mieux comprendre leur impact sur l'entraînement. Lorsque vous répondez aux questions, ***veillez à régler correctement les hyperparamètres***.\n",
    "\n",
    "A noter, ci-dessous sont les valeurs d'hyperparamètres initiales utilisées à la section 3.0 de ce notebook à utiliser comme référence:\n",
    "\n",
    "*alpha* = 0,1\n",
    "\n",
    "*gamma* = 0,5\n",
    "\n",
    "*epsilon* = 0,1\n",
    "\n",
    "*num_simulations* = 100000\n",
    "\n",
    "Pour toutes nos évaluations, nous utiliserons 500 épisodes. De plus, après chaque expérience, vous imprimerez la valeur qtable pour l'état 328, afin de pouvoir la comparer avec ce qui a été obtenu dans la question Q4 plus tôt.\n",
    "\n",
    "***ATTENTION (répété ici de Q5): Si la fonction evaluate_agent_QL semble s'exécuter trop longtemps (30s ou plus), arrêtez l'exécution en cliquant sur le bouton en haut à gauche de la cellule de code en cours d'exécution et réexécutez-la. Cela se produit parce que l'entraînement était suffisant pour définir des valeurs Q valides et a entraîné une impasse pour un état spécifique.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQxxvn0xaxrC"
   },
   "source": [
    "**(TO DO) Q6 (a) - 1.5 points**\n",
    "\n",
    "Re-entraînez l'agent en réinitialisant les valeurs d'apprentissage Q et l'entraînement pour seulement **35000 épisodes** (avec les mêmes valeurs alpha, gamma et epsilon utilisées dans la section 3.0 de ce notebook). Ensuite, effectuez un test pour 500 épisodes dans l'environnement.  Imprimez la valeur de *q_table* pour l'état 328."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxQbWXNha4EP"
   },
   "outputs": [],
   "source": [
    "# RÉPONSE Q6(a)\n",
    "# TODO: Reset q_table\n",
    "q_table = ...\n",
    "# TODO: Retrain with the specified hyperparameters\n",
    "# Hyperparameters\n",
    "alpha = ...\n",
    "gamma = ...\n",
    "epsilon = ...\n",
    "num_simulations = ...\n",
    "...\n",
    "# TODO: Test for 500 episodes\n",
    "...\n",
    "# TODO: Print q_table for state 328\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kSCmM6VFdHu"
   },
   "source": [
    "**(TO DO) Q6 (b) - 1.5 points**\n",
    "\n",
    "Re-entraînez l'agent en réinitialisant les valeurs d'apprentissage Q et en s'entraînant pour **100000 épisodes**, mais avec une **valeur epsilon de 0,8** (avec les mêmes valeurs alpha et gamma utilisées dans la section 3.0 de ce notebook). Ensuite, effectuez un test pour 500 épisodes dans l'environnement.  Imprimez la valeur de *q_table* pour l'état 328."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O0vyZ7TNFeF7"
   },
   "outputs": [],
   "source": [
    "# RÉPONSE Q6(b)\n",
    "# TODO: Reset q_table\n",
    "q_table = ...\n",
    "# TODO: Retrain with the specified hyperparameters\n",
    "# Hyperparameters\n",
    "alpha = ...\n",
    "gamma = ...\n",
    "epsilon = ...\n",
    "num_simulations = ...\n",
    "...\n",
    "# TODO: Test for 500 episodes\n",
    "...\n",
    "# TODO: Print q_table for state 328\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWcn_doxFm4H"
   },
   "source": [
    "**(TO DO) Q6 (c) - 1.5 points**\n",
    "\n",
    "Re-entraînez l'agent en réinitialisant les valeurs d'apprentissage Q et en s'entraînant pour **100000 épisodes**, mais avec une **valeur alpha de 0,7** (avec les mêmes valeurs gamma et epsilon utilisées dans la section 3.0 de ce notebook). Ensuite, effectuez un test pour 500 épisodes dans l'environnement.  Imprimez la valeur de *q_table* pour l'état 328."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSM0Ert2FnHP"
   },
   "outputs": [],
   "source": [
    "# RÉPONSE Q6(c)\n",
    "# TODO: Reset q_table\n",
    "q_table = ...\n",
    "# TODO: Retrain with the specified hyperparameters\n",
    "# Hyperparameters\n",
    "alpha = ...\n",
    "gamma = ...\n",
    "epsilon = ...\n",
    "num_simulations = ...\n",
    "...\n",
    "# TODO: Test for 500 episodes\n",
    "...\n",
    "# TODO: Print q_table for state 328\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZwH8XogFreB"
   },
   "source": [
    "**(TO DO) Q6 (d) - 1.5 points**\n",
    "\n",
    "Re-entraînez l'agent en réinitialisant les valeurs d'apprentissage Q et en entraînant pour **100000 épisodes**, mais avec une **valeur gamma de 0,15** (avec les mêmes valeurs alpha et epsilon utilisées dans la section 3.0 de ce notebook). Ensuite, effectuez un test pour 500 épisodes dans l'environnement.  Imprimez la valeur de *q_table* pour l'état 328."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVFGLvcZFrDj"
   },
   "outputs": [],
   "source": [
    "# RÉPONSE Q6(d)\n",
    "# TODO: Reset q_table\n",
    "q_table = ...\n",
    "# TODO: Retrain with the specified hyperparameters\n",
    "# Hyperparameters\n",
    "alpha = ...\n",
    "gamma = ...\n",
    "epsilon = ...\n",
    "num_simulations = ...\n",
    "...\n",
    "# TODO: Test for 500 episodes\n",
    "...\n",
    "# TODO: Print q_table for state 328\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ec4qnmyJbWYw"
   },
   "source": [
    "**(TO DO) Q6 (e) - 6 points**\n",
    "\n",
    "En utilisant les résultats obtenus à partir de vos tests en Q6 (a), (b), (c) et (d), ainsi que les résultats initiaux de Q5 (approche *baseline*), expliquez les impacts de la modification du nombre d'épisodes entraînés (moins d'épisodes vs plus d'épisodes), la valeur alpha (petite vs grande), la valeur gamma (petite vs grande) et la valeur epsilon (petite vs grande). Même si les différences dans les comparaisons sont mineures, indiquez-les.\n",
    "\n",
    "En plus de regarder les résultats obtenus (nombre moyen de *timestep*), discutez également de l'impact sur les q-values des différentes actions de l'état 328. Est-ce que celles-ci ont changé ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WA3mYBjObbMi"
   },
   "source": [
    "**RÉPONSE Q6(e)**\n",
    "\n",
    "*Assurez-vous de discuter de chacun des hyperparamètres en lien avec le nombre de timestep et les q-values.*\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKwX68N3Ruqn"
   },
   "source": [
    "**5.0 - Vers un environnement de plus grande taille**   \n",
    "\n",
    "Comme nous l'avons vu dans les vidéos du cours, et comme vous l'avez constaté ici, une limitation de Q-Learning est que les Q-Values doivent être apprises pour toutes les paires état/action. Dans notre problème-jouet du taxi autonome, nous avons travaillé avec une petite grille (5x5) et seulement 4 lieux pour prendre et déposer un passager, et nous étions déjà à 500 états, donc avec 6 actions, cela faisait 3000 paires état/action, donc 3000 q-values à apprendre.\n",
    "\n",
    "Imaginez une grille plus grande de 100x100, et imaginez que toutes les cellules de la grille pourraient être des cellules où prendre ou déposer un passager, alors Q-learning ne serait pas une approche réaliste et nous devrions passer à Deep Q-Learning, comme nous l'avons vu dans le vidéos de cours.\n",
    "\n",
    "La mise en œuvre d'une approche Deep Q-Learning dépasse le cadre de ce Notebook, mais nous pouvons tout de même penser au type d'architecture que cela nécessiterait.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmYFwKYKRvEm"
   },
   "source": [
    "**(TO DO) Q7 - 2 points**\n",
    "\n",
    "Compte tenu de cette nouvelle description d'une grille plus grande et d'un ensemble plus vaste de lieux pour prendre et déposer un passager, décrivez à quoi pourrait ressembler un réseau de neurones pour le Deep-Q-learning. Que pourrait-on mettre aux nœuds d'entrée et combien en faudrait-il ? Que devrions-nous apprendre aux nœuds de sortie et combien de nœuds de sortie seraient nécessaires?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4Ln6mTDRvQG"
   },
   "source": [
    "**RÉPONSE Q7**\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiD_vGE6jMOu"
   },
   "source": [
    "***SIGNATURE:***\n",
    "Mon nom est --------------------------.\n",
    "Mon numéro d'étudiant(e) est -----------------.\n",
    "Je confirme être l'auteur(e) de ce travail."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CSI4506_RL_Automne2021.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
